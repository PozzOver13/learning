{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQfD2v/gg6TVgLb2dtwvdL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PozzOver13/learning/blob/main/programming/20250220_design_machine_learning_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Design Machine Learning Systems by Chip Huyen\n",
        "\n",
        "# Chip Huyen\n",
        "\n",
        "Chip Huyen è una **scrittrice e computer scientist specializzata in machine learning e MLOps**. Ha lavorato presso NVIDIA e Snorkel AI e ha co-fondato Claypot AI, una startup focalizzata sull’implementazione di modelli di machine learning in tempo reale. Inoltre, ha **insegnato alla Stanford University** un corso su MLOps e sistemi di ML.\n",
        "\n",
        "Chip Huyen è nota per i suoi post e articoli su blog e social, dove condivide riflessioni su AI, MLOps e ingegneria del software applicata al machine learning."
      ],
      "metadata": {
        "id": "F8iyi8Egh6eP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Overview of Machine Learning Systems\n",
        "\n",
        "![ml_system](https://drive.google.com/uc?id=1duHY57mA7AiXmcK6sWMO2JcJiOZVdyEk)\n",
        "\n",
        "Il libro inizia presentando una definizione di un sistema di Machine Learning (ML) e proponendosi come framework per costruirlo in modo efficace.\n",
        "Ogni data scientist deve sempre ricordare che l'algoritmo e lo sviluppo di un modello predittivo rappresenta solo una piccola parte di un \"sistema di Machine Learning\". Per definirlo è possibile ragionare su 6 macro categorie presentate in questo libro, quali:\n",
        "1. Infrastructure\n",
        "1. Data\n",
        "1. Feature Engineering\n",
        "1. ML algorithms\n",
        "1. Evaluation Metrics\n",
        "1. Deployment, monitoring, update logics\n",
        "\n",
        "Questo sistema interagisce con diverse entità, come gli stakeholders di business, gli utenti e in primis con gli sviluppatori. Ognuno di questi attori porta con se i propri requisiti, che devono essere necessariamente allineati per permettere ad un sistema di ML di prosperare. Questa è sicuramente una sfida ricorrente e che solo le realtà più efficienti riesce a gestire opportunamente.\n",
        "\n",
        "Il rischio per i data scientist è di concentrarsi troppo sulla parte di sviluppo mentre potrebbe essere portato a trascurare gli aspetti di deployment o monitoring. Un esempio ricorrente è rappresentato dalle sfide computazionali e dai requisiti di latenza che potrebbero essere anche altamente vincolanti per alcuni clienti. Altri elementi forse più contigui allo sviluppo come la \"fairness\" e la \"interpretability\" di un modello si stanno affermando come requisiti necessari e vincolanti per la buona riuscita di un sistema ML. Una citazione che mi è sembrata particolarmente interessante è la seguente:\n",
        "\n",
        "> Gli algoritmi di ML non prevedono il futuro, ma codificano il passato, perpetuando così i bias presenti nei dati e oltre. Quando vengono implementati su larga scala, gli algoritmi di ML possono discriminare le persone su larga scala.\n",
        "\n",
        "Questo è un principio che occorre sempre ricordare ma che è diventato molto più evidente con l'introduzione di prodotti costruiti sui Large Language Models (LLM), che per loro costruzione metodologica sintetizzano i pregi e i difetti dei dati sui cui sono stati addestrati. Questa criticità sottolinea ancora in modo più evidente come occorra distringuere i modelli di machine learning rispetto ai software tradizionali. Non basta un codice funzionante ed efficiente ma occorre una perfetta sincronizzazione tra codice, dati e artefatti. Per questo il ruolo del data scientist è particolarmente importante sia per la responsabilità sullo sviluppo software ma soprattutto per la creazione di artefatti performanti e adattabili a delle condizioni potenzialmente in rapido mutamento e, mi permetto di aggiungere, per la diffusione della cultura del dato. Infatti, sono numerosi gli esempi che testimoniano come un aumento della quantità e qualità dei dati permette di ottenere un vantaggio competitivo rilevante.\n",
        "\n"
      ],
      "metadata": {
        "id": "LL7uX1Cb0qhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Introduction to Machine Learning Systems Design\n",
        "\n",
        "**Obiettivi di Business e ML**  \n",
        "- Quali metriche interessano alle aziende? Anche se molte aziende vogliono convincerti del contrario, secondo l’economista premio Nobel Milton Friedman, l’unico scopo delle imprese è massimizzare i profitti per gli azionisti.  \n",
        "- I ritorni sugli investimenti in ML dipendono molto dal livello di maturità nell'adozione. Più a lungo un'azienda ha adottato il ML, più efficiente sarà la pipeline, più rapido sarà il ciclo di sviluppo, meno tempo di ingegneria sarà necessario e più bassi saranno i costi del cloud, il che porta a ritorni più elevati.  \n",
        "\n",
        "**Requisiti per i Sistemi di ML**  \n",
        "Quattro requisiti fondamentali: **affidabilità, scalabilità, manutenibilità e adattabilità**.  \n",
        "\n",
        "- **Affidabilità**  \n",
        "  - Il sistema dovrebbe continuare a svolgere la funzione corretta al livello di prestazioni desiderato anche in condizioni avverse (guasti hardware/software o errori umani). La \"correttezza\" può essere difficile da definire nei sistemi ML.  \n",
        "\n",
        "- **Scalabilità**  \n",
        "  - Un sistema di ML può crescere in diversi modi:  \n",
        "    - Può aumentare in complessità.  \n",
        "    - Può gestire un volume di traffico maggiore.  \n",
        "    - Può supportare un numero crescente di modelli di ML.  \n",
        "  - Un elemento indispensabile nei servizi cloud è **l'auto-scalabilità**, ovvero la capacità di aumentare o diminuire automaticamente il numero di macchine in base all'uso. Tuttavia, questa funzionalità può essere complessa da implementare.  \n",
        "\n",
        "- **Manutenibilità**  \n",
        "  - È fondamentale strutturare i flussi di lavoro e l’infrastruttura in modo che i vari team possano usare strumenti con cui si sentono a proprio agio, evitando che un gruppo imponga le proprie scelte agli altri.  \n",
        "  - Il codice dovrebbe essere documentato.  \n",
        "  - Codice, dati e artefatti dovrebbero essere versionati.  \n",
        "  - I modelli dovrebbero essere riproducibili, in modo che chiunque possa lavorarci anche se gli autori originali non sono disponibili.  \n",
        "  - Quando si verifica un problema, i vari team dovrebbero collaborare per risolverlo senza attribuire colpe.  \n",
        "\n",
        "- **Adattabilità**  \n",
        "  - I sistemi di ML devono evolversi rapidamente per rispondere ai cambiamenti del contesto.  \n",
        "\n",
        "**Processo Iterativo**  \n",
        "![ml_lifecycle](https://drive.google.com/uc?id=1hTIX1oxNwDN8CvWDEw3tUY33-RUhcjk9)\n",
        "\n",
        "- Sviluppare un sistema ML è un processo iterativo e, nella maggior parte dei casi, senza una vera fine. Una volta che un sistema è in produzione, deve essere monitorato e aggiornato continuamente.  \n",
        "\n",
        "**Definizione dei Problemi di ML**  \n",
        "- Un’assistenza clienti lenta è un problema, ma non è un problema di ML. Un problema di ML è definito da **input, output e funzione obiettivo**, che guidano il processo di apprendimento. Nessuna di queste tre componenti è ovvia da una semplice richiesta del capo. È compito dell'ingegnere ML tradurre il problema in un caso applicabile al ML.  \n",
        "\n",
        "**Tipi di Task di ML**  \n",
        "- L’output del modello determina la tipologia del problema ML. Le due categorie principali sono **classificazione e regressione**.  \n",
        "\n",
        "**Decoupling degli Obiettivi**  \n",
        "- Quando ci sono più obiettivi, è una buona pratica separarli, perché:  \n",
        "  1. Rende più semplice lo sviluppo e la manutenzione del modello.  \n",
        "  2. Permette di modificare il sistema senza dover riaddestrare i modelli.  \n",
        "  3. Facilita la manutenzione, poiché obiettivi diversi possono avere esigenze di aggiornamento diverse.  \n",
        "\n",
        "**Mente vs Dati**  \n",
        "- Nel dibattito tra mente e dati, il Dr. **Judea Pearl**, vincitore del Turing Award e noto per i suoi studi sull'inferenza causale e le reti bayesiane, ha affermato:  \n",
        "  > \"I dati sono profondamente stupidi.\"  \n",
        "\n",
        "- **Peter Norvig**, direttore della qualità della ricerca di Google, ha enfatizzato il ruolo dei dati rispetto agli algoritmi affermando:  \n",
        "  > \"Non abbiamo algoritmi migliori. Abbiamo solo più dati.\"  \n",
        "\n",
        "- Se vuoi usare la data science (di cui il ML è una branca) per migliorare prodotti o processi, devi partire **dalla qualità e dalla quantità dei dati**.  \n",
        "- **Senza dati, non esiste data science.**"
      ],
      "metadata": {
        "id": "AiunTjJe0xb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Engineering Fundamentals\n",
        "\n",
        "![etl_process](https://drive.google.com/uc?id=1fUfU5Ap5qmNZl5t2JZYabnGVmgswAwfw)\n",
        "\n",
        "\n",
        "- Se vuoi approfondire l'ingegneria dei dati da una prospettiva sistemistica, ti consiglio l'eccellente libro di Martin Kleppmann *Designing Data-Intensive Applications* (O’Reilly, 2017).  \n",
        "\n",
        "**Fonti di Dati**  \n",
        "- I dati di **prima parte** sono quelli che la tua azienda raccoglie direttamente sugli utenti o clienti.  \n",
        "- I dati di **seconda parte** sono raccolti da un'altra azienda sui propri clienti e messi a disposizione (spesso a pagamento).  \n",
        "- I dati di **terza parte** sono raccolti da aziende che monitorano il pubblico senza avere una relazione diretta con esso.  \n",
        "\n",
        "**Formati di Dati**  \n",
        "- Il processo di conversione di una struttura dati o dello stato di un oggetto in un formato archiviabile o trasmissibile, ricostruibile in seguito, è chiamato **serializzazione dei dati**. Esistono moltissimi formati di serializzazione, e nella scelta bisogna considerare leggibilità umana, pattern di accesso, e se il formato è testuale o binario (impattando sulle dimensioni dei file).  \n",
        "- Wikipedia ha un'ottima pagina intitolata *Comparison of Data-Serialization Formats*.  \n",
        "\n",
        "**JSON**  \n",
        "- JSON (*JavaScript Object Notation*) è onnipresente. Pur derivando da JavaScript, è indipendente dal linguaggio: la maggior parte dei linguaggi moderni può generarlo e analizzarlo. È leggibile dall’uomo e utilizza un paradigma chiave-valore potente e adatto a dati con diversi livelli di struttura.  \n",
        "\n",
        "**Formato Row-Major vs Column-Major**  \n",
        "- Due formati comuni, che rappresentano due paradigmi distinti, sono **CSV** e **Parquet**.  \n",
        "  - **CSV** è **row-major**, ossia gli elementi consecutivi di una riga sono memorizzati insieme in memoria.  \n",
        "  - **Parquet** è **column-major**, ossia gli elementi consecutivi di una colonna sono memorizzati insieme.  \n",
        "- I formati **row-major** sono migliori per scritture frequenti, mentre quelli **column-major** sono più efficienti nelle letture colonnari.  \n",
        "- Accedere a un DataFrame per riga è molto più lento che accedervi per colonna. Tuttavia, convertendo lo stesso DataFrame in un `ndarray` di NumPy, l’accesso per riga diventa molto più veloce.  \n",
        "\n",
        "**Formato Testuale vs Binario**  \n",
        "- **CSV e JSON** sono file di **testo**, mentre **Parquet** è un file **binario**.  \n",
        "- I file testuali sono leggibili dall’uomo, mentre i file binari sono destinati a essere interpretati da programmi che leggono byte grezzi.  \n",
        "- **AWS consiglia il formato Parquet** perché permette uno scaricamento fino a **2 volte più veloce** e consuma fino a **6 volte meno spazio** rispetto ai formati testuali su Amazon S3.  \n",
        "\n",
        "**Modelli di Dati**  \n",
        "**Modello Relazionale**  \n",
        "- I dati sono organizzati in **relazioni** (insiemi di tuple). Una tabella è la rappresentazione visiva di una relazione, e ogni riga è una tupla.  \n",
        "- La **normalizzazione** sparge i dati tra più relazioni: questo riduce la ridondanza ma rende costose le operazioni di join su tabelle grandi.  \n",
        "- **SQL è un linguaggio dichiarativo**, a differenza di Python che è imperativo.  \n",
        "  - In un linguaggio **imperativo**, si specificano i passaggi per ottenere un risultato.  \n",
        "  - In un linguaggio **dichiarativo**, si specifica solo il risultato desiderato e il sistema trova i passaggi necessari.  \n",
        "- Esempi di framework ML dichiarativi: **Ludwig** (Uber) e **H2O AutoML**.  \n",
        "\n",
        "**NoSQL**  \n",
        "- Inizialmente il termine NoSQL era un hashtag per incontri sui database non relazionali, poi è stato reinterpretato come *Not Only SQL*.  \n",
        "- I due principali modelli NoSQL sono:  \n",
        "  - **Modello a documenti**, utile quando i dati sono contenuti in documenti indipendenti con relazioni rare.  \n",
        "  - **Modello a grafo**, utile quando le relazioni tra elementi sono comuni e importanti.  \n",
        "\n",
        "**Modello a Documenti**  \n",
        "- Nei database a documenti, la struttura viene determinata dal sistema che legge i dati anziché da quello che li scrive.  \n",
        "- Molti database, come **PostgreSQL e MySQL**, supportano sia il modello relazionale che quello a documenti.  \n",
        "\n",
        "**Modello a Grafo**  \n",
        "- Usato per dati con molte relazioni complesse e connesse.  \n",
        "\n",
        "**Dati Strutturati vs Non Strutturati**  \n",
        "- Un **data warehouse** memorizza dati strutturati.  \n",
        "- Un **data lake** conserva dati non strutturati (spesso grezzi, da processare successivamente).  \n",
        "\n",
        "**Motori di Archiviazione e Elaborazione**  \n",
        "**Elaborazione Transazionale vs Analitica**  \n",
        "- **Database transazionali** sono progettati per gestire transazioni con **bassa latenza e alta disponibilità**.  \n",
        "- Quando si parla di database transazionali, si pensa subito alle proprietà **ACID** (Atomicità, Consistenza, Isolamento, Durabilità).  \n",
        "- **OLTP vs OLAP**  \n",
        "  - **OLTP** (Online Transaction Processing): ottimizzato per operazioni rapide e frequenti su pochi record.  \n",
        "  - **OLAP** (Online Analytical Processing): ottimizzato per analisi su grandi quantità di dati.  \n",
        "\n",
        "**ETL: Extract, Transform, Load**  \n",
        "- ETL è il processo generale di estrazione, trasformazione e caricamento dei dati nel formato desiderato.  \n",
        "\n",
        "**Modalità di Flusso dei Dati**  \n",
        "**Dati che Passano Attraverso Database**  \n",
        "- Entrambi i processi accedono ai dati via database, ma letture/scritture possono essere lente e inadatte per applicazioni con requisiti di latenza stretti (come le app consumer).  \n",
        "\n",
        "**Dati che Passano Attraverso Servizi**  \n",
        "- Due principali stili di richiesta:  \n",
        "  - **REST** (*Representational State Transfer*): progettato per richieste su rete.  \n",
        "  - **RPC** (*Remote Procedure Call*): cerca di rendere una richiesta di rete simile a una chiamata a funzione locale.  \n",
        "- **REST è dominante nelle API pubbliche**, mentre **RPC** è più usato per richieste interne tra servizi della stessa organizzazione.  \n",
        "\n",
        "**Dati che Passano Attraverso Trasporto in Tempo Reale**  \n",
        "- Invece di usare database come intermediari, si usa la **memoria**.  \n",
        "- Il **trasporto in tempo reale** può essere visto come una memoria per lo scambio di dati tra servizi.  \n",
        "- Un dato trasmesso nel sistema si chiama **evento**, per questo l’architettura è anche detta **event-driven**.  \n",
        "- Un sistema di trasporto in tempo reale è chiamato **event bus**.  \n",
        "\n",
        "**Elaborazione in Batch vs Stream Processing**  \n",
        "- **Batch processing**: l’elaborazione avviene su dati accumulati in blocchi.  \n",
        "  - Sistemi come **MapReduce e Spark** sono progettati per elaborare dati batch in modo efficiente.  \n",
        "- **Stream processing**: i dati vengono elaborati in tempo reale mentre fluiscono attraverso sistemi come **Apache Kafka e Amazon Kinesis**.  \n",
        "  - Può essere eseguito periodicamente, ma con intervalli più brevi rispetto al batch processing.  "
      ],
      "metadata": {
        "id": "ENyP8MsEiaI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 4. Training Data\n",
        "## 5. Feature Engineering\n",
        "## 6. Model Development and Offline Evaluation\n",
        "\n"
      ],
      "metadata": {
        "id": "yKlnfR7QWkIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Model Deployment and Prediction Service\n",
        "\n",
        "**Introduzione**  \n",
        "- In molte aziende, la responsabilità di distribuire i modelli ricade sugli stessi sviluppatori che li hanno creati. In molte altre, una volta che un modello è pronto per il deployment, viene esportato e consegnato a un altro team per l’implementazione. Tuttavia, questa separazione di responsabilità può causare un’elevata comunicazione tra i team e rallentare l’aggiornamento del modello.  \n",
        "- Il processo di generazione delle previsioni è chiamato **inferenza**.  \n",
        "\n",
        "**Miti sul Deployment dei Modelli di Machine Learning**  \n",
        "**Mito 1: Si Distribuiscono Solo Uno o Due Modelli ML alla Volta**  \n",
        "- In realtà, Uber ha migliaia di modelli in produzione. In qualsiasi momento, Google ha migliaia di modelli in fase di addestramento, alcuni con centinaia di miliardi di parametri. Booking.com utilizza oltre 150 modelli. Uno studio del 2021 di Algorithmia mostra che tra le organizzazioni con più di 25.000 dipendenti, il 41% ha più di 100 modelli in produzione.  \n",
        "\n",
        "**Mito 2: Se Non Facciamo Nulla, le Prestazioni del Modello Rimangono Invariate**  \n",
        "- I sistemi di machine learning soffrono di quelli che vengono chiamati **cambiamenti nella distribuzione dei dati**, ovvero situazioni in cui la distribuzione dei dati incontrata in produzione è diversa da quella su cui il modello è stato addestrato.  \n",
        "\n",
        "**Mito 3: Non È Necessario Aggiornare Spesso i Modelli**  \n",
        "- Poiché le prestazioni di un modello decadono nel tempo, è importante aggiornarlo il più rapidamente possibile. Questo è un aspetto del ML in cui possiamo imparare dalle migliori pratiche del DevOps. Già nel 2015, aziende come Etsy rilasciavano aggiornamenti 50 volte al giorno, Netflix migliaia di volte al giorno e AWS ogni 11,7 secondi.  \n",
        "\n",
        "**Mito 4: La Maggior Parte degli Ingegneri ML Non Deve Preoccuparsi della Scalabilità**  \n",
        "\n",
        "**Predizione in Batch vs. Predizione Online**  \n",
        "- Una decisione fondamentale che influenzerà sia gli utenti finali che gli sviluppatori è il modo in cui il sistema genera e serve le previsioni: **online** o **batch**.  \n",
        "- Tradizionalmente, nella predizione online, le richieste vengono inviate al servizio di previsione tramite API RESTful (es. richieste HTTP). Quando le richieste di previsione vengono inviate via HTTP, si parla anche di **predizione sincrona**, poiché le previsioni vengono generate in tempo reale rispetto alle richieste.  \n",
        "- La **predizione batch** è anche nota come **predizione asincrona**, poiché le previsioni vengono generate in modo asincrono rispetto alle richieste.  \n",
        "- I termini \"predizione online\" e \"predizione batch\" possono creare confusione, perché entrambi possono elaborare più campioni alla volta (batch) o un campione per volta. Per evitare questa ambiguità, a volte si preferiscono i termini **\"predizione sincrona\"** e **\"predizione asincrona\"**.  \n",
        "\n",
        "**Dalla Predizione Batch alla Predizione Online**  \n",
        "- Si esporta il modello, lo si carica su Amazon SageMaker o Google App Engine e si ottiene un endpoint esposto. Ora, se si invia una richiesta contenente un input a quell’endpoint, si riceverà una previsione generata su quell’input.  \n",
        "- La predizione batch è spesso un’alternativa quando la predizione online è troppo costosa o non abbastanza veloce.  \n",
        "- Con hardware sempre più specializzati e potenti, e con lo sviluppo di tecniche più efficienti per rendere le predizioni online più veloci ed economiche, la predizione online potrebbe diventare lo standard.  \n",
        "\n",
        "**Unificazione delle Pipeline Batch e Streaming**  \n",
        "- La costruzione di infrastrutture che unificano l’elaborazione in streaming e in batch è diventata un tema molto discusso nella comunità del machine learning negli ultimi anni. Aziende come Uber e Weibo hanno effettuato importanti revisioni infrastrutturali per unificare le loro pipeline di elaborazione batch e streaming utilizzando processori di flusso come **Apache Flink**.  \n",
        "- Alcune aziende utilizzano **feature store** per garantire la coerenza tra le caratteristiche batch utilizzate nell'addestramento e le caratteristiche in streaming usate in produzione.\n",
        "\n",
        "![ml_workflow](https://drive.google.com/uc?id=1tfjNSt2OtvHvEuX4wiklGW96KdgLhxiv)\n",
        "\n",
        "**Model Compression**\n",
        "\n",
        "**Low-Rank Factorization**\n",
        "\n",
        "**Knowledge Distillation**\n",
        "\n",
        "**Pruning**\n",
        "\n",
        "**Quantization**\n",
        "\n",
        "\n",
        "**ML nel Cloud e all’Edge**  \n",
        "- Una decisione chiave riguarda dove avverrà l’elaborazione del modello: **nel cloud** o **all’edge**.  \n",
        "  - **Nel cloud** significa che gran parte dell’elaborazione viene eseguita su infrastrutture cloud, sia pubbliche che private.  \n",
        "  - **All’edge** significa che gran parte dell’elaborazione viene eseguita direttamente sui dispositivi dell’utente, come browser, telefoni, laptop, smartwatch, automobili, telecamere di sicurezza, robot, dispositivi embedded, FPGA (field-programmable gate arrays) e ASIC (application-specific integrated circuits), noti anche come **dispositivi edge**.  \n",
        "- Se i modelli sono ospitati su **cloud pubblici**, dipendono da connessioni Internet stabili per inviare e ricevere dati. **L’edge computing**, invece, consente ai modelli di funzionare anche in assenza di connessione o con connessioni instabili, ad esempio in aree rurali o in paesi in via di sviluppo.  \n",
        "- L’elaborazione su cloud implica spesso l’archiviazione di dati di molti utenti nello stesso luogo, aumentando il rischio che una violazione della sicurezza possa colpire più persone.  \n",
        "\n",
        "**Compilazione e Ottimizzazione dei Modelli per Dispositivi Edge**  \n",
        "- **Ottimizzazione dei modelli**  \n",
        "- **Uso del Machine Learning per ottimizzare i modelli ML**  \n",
        "- **ML nei browser**  "
      ],
      "metadata": {
        "id": "5TD94YvnELk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Data Distribution Shifts and Monitoring\n",
        "## 9. Continual Learning and Test in Production"
      ],
      "metadata": {
        "id": "3zsWbSybEMff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 10. Infrastructure and Tooling for MLOps\n",
        "\n",
        "![infrastructure](https://drive.google.com/uc?id=147yWgCiShtZoEv0eKABX9aZNrfMu1hMC)\n",
        "\n",
        "\n",
        "Nel mondo del ML, l'infrastruttura è l'insieme delle strutture fondamentali che supportano lo sviluppo e la manutenzione dei sistemi di ML.**  \n",
        "\n",
        "**Storage e Compute**  \n",
        "- Nella sua forma più semplice, lo strato di storage può essere un hard disk (HDD) o un disco a stato solido (SSD). Può essere centralizzato, ad esempio con tutti i dati in Amazon S3 o Snowflake, oppure distribuito su più posizioni. Lo storage può essere on-premise in un data center privato o nel cloud.  \n",
        "- Lo strato di calcolo (compute) si riferisce a tutte le risorse computazionali a cui un'azienda ha accesso e al meccanismo per determinarne l'utilizzo.  \n",
        "- Nella sua forma più semplice, lo strato di compute può essere un singolo core CPU o GPU. La forma più comune è il calcolo gestito da un provider cloud come AWS EC2 o GCP.  \n",
        "- Alcuni motori di calcolo, come Spark e Ray, utilizzano \"job\" come unità di computazione, mentre Kubernetes utilizza i \"pod\", che sono contenitori eseguibili.  \n",
        "- Un'unità di calcolo si caratterizza principalmente per due metriche: la quantità di memoria disponibile e la velocità di esecuzione delle operazioni.  \n",
        "\n",
        "**Cloud pubblico vs Data Center privato**  \n",
        "- Il cloud permette alle aziende di iniziare rapidamente senza preoccuparsi dell'infrastruttura. È particolarmente vantaggioso per carichi di lavoro variabili.  \n",
        "- Il cloud semplifica la scalabilità delle risorse computazionali, riducendo il carico operativo degli ingegneri. Questo è utile nel ML, dove le esigenze di calcolo variano: durante lo sviluppo gli esperimenti richiedono molta potenza, mentre in produzione il carico è più stabile.  \n",
        "- Secondo un'analisi di a16z, il costo dell'infrastruttura cloud rappresenta circa il 50% dei costi di revenue per molte aziende software.  \n",
        "- a16z ha stimato che 50 tra le principali aziende software pubbliche stanno perdendo circa 100 miliardi di dollari di valore di mercato a causa dell'impatto del cloud sui margini, rispetto a un'infrastruttura gestita internamente.  \n",
        "\n",
        "**Ambiente di sviluppo (Dev Environment)**  \n",
        "L’ambiente di sviluppo include i seguenti componenti: IDE (Integrated Development Environment), versionamento e CI/CD.  \n",
        "\n",
        "**Setup dell’ambiente di sviluppo**  \n",
        "- Le aziende utilizzano strumenti come Git per il versionamento del codice, DVC per il versionamento dei dati, Weights & Biases o Comet.ml per tracciare esperimenti e MLflow per gestire gli artefatti dei modelli in fase di deployment.  \n",
        "- Strumenti per orchestrare la CI/CD includono GitHub Actions e CircleCI.  \n",
        "\n",
        "**IDE**  \n",
        "- I notebook non servono solo per scrivere codice, ma anche per integrare immagini, grafici e tabelle, rendendoli molto utili per l'analisi esplorativa e la valutazione dei risultati di addestramento.  \n",
        "- Netflix, nel suo articolo \"Beyond Interactive: Notebook Innovation at Netflix\", ha mostrato strumenti infrastrutturali per potenziare i notebook.  \n",
        "\n",
        "**Standardizzazione degli ambienti di sviluppo**  \n",
        "- VS Code è una buona scelta perché permette un'integrazione facile con le istanze cloud.  \n",
        "- Alcune aziende utilizzano GitHub Codespaces come ambiente di sviluppo cloud, mentre AWS EC2 o GCP con SSH sono alternative valide.  \n",
        "- Avere l’ambiente di sviluppo nel cloud riduce il divario tra sviluppo e produzione.  \n",
        "\n",
        "**Dallo sviluppo alla produzione: i container**  \n",
        "- Per replicare un ambiente su una nuova istanza, si utilizzano i container. Docker è la tecnologia più diffusa per farlo, permettendo di creare un **Dockerfile** con istruzioni dettagliate per l’ambiente (installazione di pacchetti, modelli pre-addestrati, variabili di ambiente, ecc.).  \n",
        "- Due concetti chiave di Docker:  \n",
        "  - **Image**: l'insieme delle istruzioni del Dockerfile eseguite.  \n",
        "  - **Container**: un’istanza in esecuzione di un'immagine Docker.  \n",
        "- **Orchestrazione dei container**:  \n",
        "  - Docker Compose è un orchestratore leggero per la gestione di più container su un singolo host.  \n",
        "  - Kubernetes (K8s) è uno strumento per orchestrare container su più istanze, permettendo scalabilità e alta disponibilità.  \n",
        "\n",
        "**Gestione delle risorse**  \n",
        "\n",
        "**Cron, Scheduler e Orchestrator**  \n",
        "- Gli scheduler determinano **quando** eseguire un job e quali risorse sono necessarie.  \n",
        "- Gli orchestratori decidono **dove** ottenere tali risorse.  \n",
        "- Kubernetes è l'orchestratore più noto per la gestione dei container.  \n",
        "\n",
        "**Gestione del workflow in Data Science**  \n",
        "- Strumenti come **Airflow, Argo, Prefect, Kubeflow e Metaflow** permettono la gestione dei flussi di lavoro.  \n",
        "- I workflow possono essere definiti come **DAG** (grafi aciclici diretti) e comprendono tipicamente fasi di **featurization, training e valutazione del modello**.  \n",
        "- I workflow possono essere scritti in Python o YAML.  \n",
        "- Ogni fase del workflow è un **task**.  \n",
        "\n",
        "\n",
        "**ML Platform**  \n",
        "\n",
        "**Model Deployment**  \n",
        "- Se il modello esegue **predizioni online**, viene esposto tramite un endpoint che genera una predizione su richiesta.  \n",
        "- Se il modello esegue **predizioni batch**, l’endpoint recupera predizioni precomputate.  \n",
        "- **Soluzioni di deployment più diffuse**:  \n",
        "  - **Cloud provider**: AWS SageMaker, GCP Vertex AI, Azure ML, Alibaba Machine Learning Studio.  \n",
        "  - **Startup e strumenti indipendenti**: MLflow Models, Seldon, Cortex, Ray Serve.  \n",
        "- È importante valutare la facilità di gestione sia per predizioni online che batch.  \n",
        "\n",
        "**Model Store**  \n",
        "- **MLflow è attualmente il model store più popolare** tra quelli non legati a provider cloud.  \n",
        "- Molte aziende hanno capito che archiviare solo il modello non è sufficiente: è fondamentale tracciare anche altre informazioni per il debugging e la manutenzione, tra cui:  \n",
        "  - Definizione del modello  \n",
        "  - Parametri  \n",
        "  - Funzioni di featurization e predizione  \n",
        "  - Dipendenze  \n",
        "  - Dati  \n",
        "  - Codice di generazione del modello  \n",
        "  - Artefatti degli esperimenti  \n",
        "  - Tag  \n",
        "- Il problema del model store è ancora aperto e potrebbe emergere una startup per risolverlo.  \n",
        "\n",
        "**Feature Store**  \n",
        "- Il concetto di \"Feature Store\" varia a seconda di chi lo utilizza. In generale, un Feature Store aiuta a gestire:  \n",
        "  - **Feature Management** (gestione delle feature)  \n",
        "  - **Feature Transformation** (trasformazione delle feature)  \n",
        "  - **Feature Consistency** (coerenza tra training e inferenza)  \n",
        "- Alcuni esempi di strumenti di Feature Store:  \n",
        "  - **Amundsen e Datahub**  \n",
        "  - **Feast, Tecton, SageMaker Feature Store, Databricks Feature Store**  \n",
        "- Un feature store funziona come un **data warehouse** per le feature di ML.  \n",
        "\n",
        "**Build vs Buy**  \n",
        "- **Costruire un ML platform internamente** offre maggiore flessibilità e controllo, ma richiede notevoli investimenti in tempo e risorse.  \n",
        "- **Acquistare soluzioni preconfezionate** può accelerare il time-to-market e ridurre i costi operativi, ma può limitare la personalizzazione.  \n",
        "\n"
      ],
      "metadata": {
        "id": "cwGbtTVMNBil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. The Human Side of Machine Learning\n",
        "\n",
        "**Esperienza Utente**\n",
        "- **Garantire Coerenza nell’Esperienza Utente**\n",
        "    - Per i compiti che vogliono sfruttare il Machine Learning per migliorare l’esperienza degli utenti, l’incoerenza nelle predizioni del modello può rappresentare un ostacolo.\n",
        "  \n",
        "- **Contrastare le Predizioni “Per lo più Corrette”**\n",
        "    - Questo approccio è molto comune e viene talvolta chiamato “AI con l’uomo nel loop” (“human-in-the-loop”), poiché coinvolge esseri umani per selezionare le migliori predizioni o per migliorare quelle generate dalla macchina.\n",
        "\n",
        "- **Degradazione Graduale delle Prestazioni (Smooth Failing)**\n",
        "    - Questo concetto è legato al compromesso tra velocità e accuratezza: un modello potrebbe avere prestazioni inferiori rispetto a un altro, ma essere in grado di effettuare inferenze molto più rapidamente. Questo modello meno ottimale ma più veloce potrebbe fornire predizioni peggiori agli utenti, ma essere comunque preferibile in situazioni in cui la latenza è cruciale.\n",
        "\n",
        "**Struttura del Team**\n",
        "\n",
        "![e2edatascientist](https://drive.google.com/uc?id=1HDN6KrldIuDixWen4_de8yQ9iGW9Zq4J)\n",
        "\n",
        "- **Collaborazione tra Team Cross-funzionali**\n",
        "    - Gli esperti del dominio (SME – Subject Matter Experts) non sono solo utenti, ma anche sviluppatori di sistemi di Machine Learning.\n",
        "    - Un sistema ML trarrebbe grande beneficio dal coinvolgimento degli SME anche nelle fasi successive del ciclo di vita: formulazione del problema, ingegneria delle caratteristiche, analisi degli errori, valutazione del modello, riordinamento delle predizioni e interfaccia utente – ovvero su come presentare al meglio i risultati agli utenti e/o ad altre parti del sistema.\n",
        "\n",
        "- **Data Scientist End-to-End**\n",
        "    - **Approccio 1: Team separato per la gestione della produzione**\n",
        "        - In questo approccio, il team di data science/ML sviluppa i modelli nell’ambiente di sviluppo. Poi un team separato, solitamente il team Ops/piattaforma/ML engineering, mette in produzione i modelli.\n",
        "        - Sfide e svantaggi:\n",
        "            - Sovraccarico nella comunicazione e nel coordinamento\n",
        "            - Difficoltà nel debugging\n",
        "            - Scarico delle responsabilità (scaricabarile)\n",
        "            - Contesto limitato\n",
        "\n",
        "    - **Approccio 2: I data scientist gestiscono l’intero processo**\n",
        "        - In questo approccio, anche la messa in produzione dei modelli è responsabilità del team di data science. I data scientist diventano “unicorni\", a cui si richiede di conoscere tutto il processo, finendo per scrivere più codice ripetitivo (boilerplate) che codice di data science.\n",
        "        - “Il potere del data scientist generalista full-stack e i pericoli della divisione del lavoro per funzione.”\n",
        "        - \"Sono entrato in un’azienda di ML perché volevo passare più tempo con i dati, non con l’avvio di istanze AWS, la scrittura di Dockerfile, la pianificazione e lo scaling dei cluster, o il debugging di file di configurazione YAML.\"\n",
        "        - Secondo Stitch Fix e Netflix, il successo di un data scientist full-stack dipende dagli strumenti che ha a disposizione."
      ],
      "metadata": {
        "id": "tY8y9tc9NCCL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80Ws-vIohu29"
      },
      "outputs": [],
      "source": []
    }
  ]
}