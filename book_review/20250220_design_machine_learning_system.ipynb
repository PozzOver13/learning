{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN9F/pRlrJNEXn7TLREl5el",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PozzOver13/learning/blob/main/book_review/20250220_design_machine_learning_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Design Machine Learning Systems by Chip Huyen"
      ],
      "metadata": {
        "id": "Gwt5T2QWj12c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Chip Huyen\n",
        "\n",
        "Chip Huyen è una **scrittrice e computer scientist specializzata in machine learning e MLOps**. Ha lavorato presso NVIDIA e Snorkel AI e ha co-fondato Claypot AI, una startup focalizzata sull’implementazione di modelli di machine learning in tempo reale. Inoltre, ha **insegnato alla Stanford University** un corso su MLOps e sistemi di ML.\n",
        "\n",
        "Chip Huyen è nota per i suoi post e articoli su blog e social, dove condivide riflessioni su AI, MLOps e ingegneria del software applicata al machine learning."
      ],
      "metadata": {
        "id": "F8iyi8Egh6eP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Overview of Machine Learning Systems\n",
        "\n",
        "![ml_system](https://drive.google.com/uc?id=1duHY57mA7AiXmcK6sWMO2JcJiOZVdyEk)\n",
        "\n",
        "Il libro inizia presentando una definizione di un sistema di Machine Learning (ML) e **proponendosi come framework** per costruirlo in modo efficace.\n",
        "Ogni data scientist deve sempre ricordare che l'algoritmo e lo sviluppo di un modello predittivo rappresenta solo una piccola parte di un \"sistema di Machine Learning\". Per definirlo è possibile ragionare su 6 macro categorie presentate in questo libro, quali:\n",
        "1. Infrastructure\n",
        "1. Data\n",
        "1. Feature Engineering\n",
        "1. ML algorithms\n",
        "1. Evaluation Metrics\n",
        "1. Deployment, monitoring, update logics\n",
        "\n",
        "Questo sistema interagisce con diverse entità, come gli stakeholders di business, gli utenti e in primis con gli sviluppatori. **Ognuno di questi attori porta con se i propri requisiti**, che devono essere necessariamente allineati per permettere ad un sistema di ML di prosperare. Questa è sicuramente una sfida ricorrente e che solo le realtà più efficienti riesce a gestire opportunamente.\n",
        "\n",
        "Il **rischio per i data scientist** è di concentrarsi troppo sulla parte di sviluppo mentre potrebbe essere portato a trascurare gli aspetti di deployment o monitoring. Un esempio ricorrente è rappresentato dalle sfide computazionali e dai requisiti di latenza che potrebbero essere anche altamente vincolanti per alcuni clienti. Altri elementi forse più contigui allo sviluppo come la \"fairness\" e la \"interpretability\" di un modello si stanno affermando come requisiti necessari e vincolanti per la buona riuscita di un sistema ML. Una citazione che mi è sembrata particolarmente interessante è la seguente:\n",
        "\n",
        "> Gli algoritmi di ML non prevedono il futuro, ma codificano il passato, perpetuando così i bias presenti nei dati e oltre. Quando vengono implementati su larga scala, gli algoritmi di ML possono discriminare le persone su larga scala.\n",
        "\n",
        "Questo è un principio che occorre sempre ricordare ma che è diventato molto più evidente con l'introduzione di prodotti costruiti sui Large Language Models (LLM), che per loro costruzione metodologica sintetizzano i pregi e i difetti dei dati sui cui sono stati addestrati. Questa criticità sottolinea ancora in modo più evidente come occorra **distringuere i modelli di machine learning rispetto ai software tradizionali**. Non basta un codice funzionante ed efficiente ma occorre una perfetta sincronizzazione tra codice, dati e artefatti. Per questo il ruolo del data scientist è particolarmente importante sia per la responsabilità sullo sviluppo software ma soprattutto per la creazione di artefatti performanti e adattabili a delle condizioni potenzialmente in rapido mutamento e, mi permetto di aggiungere, per la diffusione della cultura del dato. Infatti, sono numerosi gli esempi che testimoniano come un aumento della quantità e qualità dei dati permette di ottenere un vantaggio competitivo rilevante.\n",
        "\n"
      ],
      "metadata": {
        "id": "LL7uX1Cb0qhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Introduction to Machine Learning Systems Design\n",
        "\n",
        "![ml_lifecycle](https://drive.google.com/uc?id=1hTIX1oxNwDN8CvWDEw3tUY33-RUhcjk9)\n",
        "\n",
        "Una volta definito dall'alto le componenti di un sistema di machine learning occorre passare alla parte più operativa. Questa parte operativa si suddivide in ciclo di attività connesse tra di loro, la cosiddetta **\"machine learning lifecycle\"**.   \n",
        "**Incrementare fatturato, ridurre i costi e migliorare i margini** sono evidentemente gli unici veri obbiettivi delle aziende. Tuttavia, non è possibile creare un unico sistema che gestisca tutti questi obbiettivi quindi solitamente vengono definiti obbiettivi più specifici e connessi a singoli prodotti o funzioni, che quindi hanno i loro specifici obbiettivi. La parte interessante però si trova nel **ritorno degli investimenti (ROI)**: questa metrica dipende fortemente dalla maturità nell'adozione di questi sistemi perché influenzerà l'efficienza della pipeline, la rapidità del ciclo di sviluppo, del tempo di ingegneria necessario e i costi del cloud, il che porta a ritorni più elevati.\n",
        "\n",
        "$ ROI = Pipeline(rapidità\\_sviluppo, tempo\\_ingegneria, costi\\_cloud) $\n",
        "\n",
        "Per definire un sistema di machine learning soddisfacente occorre che soddisfi 4 principi: **Affidabilità, Scalabilità, Manutenibilità, Adattabilità**. Questo significa che un sistema dovrebbe ridurre al limite i malfunzionamenti dovuti ad hardware o software ed errori umani. Dovrebbe essere flessibile per poter gestire carichi maggiori, maggiori complessità e un maggiore numero di modelli predittivi, spesso però questa parte è collegata alla scelta del cloud provider. Successivamente, scegliere strumenti adatti al monitoraggio, versionamenti di codice, dati e artefatti, oltre ad una documentazione efficiente e dei processi tra diversi team che non creino frizioni e ritardi. Infine, la particolarità di questi sistemi risiede nella possibilità di evolvere al cambiamento del contesto in cui operano soprattutto quelli particolarmente dinamici.\n",
        "\n",
        "Per un data scientist la parte spesso sottovalutata la parte di **problem framing** in quanto le richieste di business possono essere ampie e imprecise a piacere ma è responsabilità degli sviluppatori trasformare la richiesta in un problema di data science o in alternativa bloccare gli sviluppi per mancanza di elementi o dati. Un altro punto rilevante in questo passaggio è la necessità di considerare i problemi di machine learning come problemi articolati che spesso conviene scomporre in sotto problemi (**Decoupling degli Obiettivi**) in modo da poterli affrontare più agilmente, scalarli a persone diverse del team e successivamente gestirli più comodamente.\n",
        "\n",
        "Quello che invece va ricordato costantemente è che la **qualità e quantità dei dati** sono spesso la chiave dei sistemi migliori. Sicuramente il valore aggiunto di un data scientist è alto ma i dati sono il carburante per alimentare anche i motori migliori. Il tema della quantità dei dati e della sua rilevanza è particolarmente evidente ora con i recenti sviluppi degli LLM. Tuttavia, è un **dibattito** che durerà per sempre e che occorre continuare a monitorare ed essere capaci a declinare in base alla realtà in cui si opera.\n",
        "\n",
        "  > \"I dati sono profondamente stupidi.\"  \n",
        "  *Dr. Judea Pearl, vincitore del Turing Award*\n",
        "\n",
        "  > \"Non abbiamo algoritmi migliori. Abbiamo solo più dati.\"  \n",
        "  *Peter Norvig, direttore della ricerca di Google*\n",
        "  \n",
        ">  **\"Senza dati, non esiste data science.\"**"
      ],
      "metadata": {
        "id": "AiunTjJe0xb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Engineering Fundamentals\n",
        "\n",
        "![etl_process](https://drive.google.com/uc?id=1fUfU5Ap5qmNZl5t2JZYabnGVmgswAwfw)\n",
        "\n",
        "Il processo **ETL (Extract, Transform, Load)** è fondamentale per gestire i dati, permettendone l'estrazione, trasformazione e caricamento nel formato desiderato. A seconda delle esigenze, i dati possono fluire attraverso database, servizi o sistemi di trasporto in tempo reale. L'uso dei database come intermediari garantisce persistenza ma può introdurre latenza, mentre le API **REST** e **RPC** consentono una comunicazione più diretta tra servizi. Per applicazioni con requisiti di bassa latenza, il **trasporto in tempo reale** tramite **event bus** è una soluzione efficace, abilitando un’architettura **event-driven**.  \n",
        "\n",
        "L’elaborazione dei dati può avvenire in **batch** o in **streaming**. Il **batch processing** è adatto per analisi su grandi volumi di dati accumulati, sfruttando strumenti come **MapReduce e Spark**. Lo **stream processing**, invece, permette l’analisi in tempo reale con sistemi come **Apache Kafka e Amazon Kinesis**, garantendo aggiornamenti più frequenti e una reattività maggiore nelle applicazioni data-driven.\n",
        "\n",
        "**Fonti**  \n",
        "Il libro suddivide i dati in tre categorie principali a seconda della loro origine: **dati di prima parte**, raccolti direttamente dall’azienda sui propri utenti o clienti; **dati di seconda parte**, acquisiti da un'altra azienda che li ha raccolti sui propri clienti e li rende disponibili, spesso a pagamento; e **dati di terza parte**, ottenuti da aziende che monitorano il pubblico senza una relazione diretta con esso. Nonostante accetti questa suddivisione, spesso mi è comodo ragionare come dati proprietari (1-2) e di terza parte (2-3).\n",
        "\n",
        "Una volta ottenuti, i dati devono essere gestiti in formati adeguati, un processo che implica la **serializzazione**, ossia la conversione della loro struttura in un formato archiviabile o trasmissibile. La scelta del formato dipende da diversi fattori, come la leggibilità, il pattern di accesso e il tipo di rappresentazione (testuale o binaria). Un formato molto diffuso è **JSON**, che grazie alla sua struttura chiave-valore e alla sua indipendenza dal linguaggio di programmazione, è ampiamente utilizzato per lo scambio di dati.  \n",
        "\n",
        "Un altro aspetto fondamentale è la modalità di memorizzazione dei dati, che può seguire il paradigma **row-major** o **column-major**. Il formato **CSV**, ad esempio, memorizza i dati riga per riga ed è più adatto a operazioni di scrittura frequente. Al contrario, il formato **Parquet** organizza i dati per colonne, garantendo una maggiore efficienza nelle letture analitiche e nel risparmio di spazio. Inoltre, il tipo di formato influisce sulle prestazioni e sulla compatibilità con determinati sistemi: i file **testuali** (come CSV e JSON) sono leggibili dall’uomo, mentre quelli **binari** (come Parquet) sono ottimizzati per l’elaborazione da parte dei programmi. In particolare, **AWS consiglia il formato Parquet**, che consente di ridurre i tempi di scaricamento e il consumo di spazio rispetto ai formati testuali su Amazon S3, rendendolo una scelta vantaggiosa per l’archiviazione e l’analisi di grandi volumi di dati.\n",
        "\n",
        "**Obbiettivi**  \n",
        "I dati possono essere organizzati secondo diversi modelli, ognuno con vantaggi specifici in base al tipo di applicazione. Il **modello relazionale** rappresenta le informazioni in tabelle, dove ogni riga è una tupla e ogni colonna corrisponde a un attributo. Questo approccio consente una gestione rigorosa dei dati grazie alla **normalizzazione**, che riduce la ridondanza distribuendo le informazioni su più tabelle. Tuttavia, ciò può rendere le operazioni di **join** costose in termini di performance, soprattutto con dataset di grandi dimensioni. Il linguaggio utilizzato per interrogare i database relazionali è **SQL**, che segue un approccio **dichiarativo**: l’utente specifica il risultato desiderato e il sistema determina il modo migliore per ottenerlo, a differenza dei linguaggi **imperativi** come Python, in cui si definiscono esplicitamente i passaggi da eseguire.  \n",
        "\n",
        "Oltre ai database relazionali, esistono approcci **NoSQL**, inizialmente nati come alternativa ai sistemi basati su SQL, ma successivamente reinterpretati come **Not Only SQL**, per evidenziare la loro complementarità con il modello relazionale. Tra i principali modelli NoSQL troviamo quello **a documenti**, che memorizza i dati in strutture semi-strutturate (come JSON), e quello **a grafo**, che è particolarmente utile quando le relazioni tra elementi sono complesse e frequenti. Molti database moderni, come **PostgreSQL** e **MySQL**, supportano sia il modello relazionale sia quello a documenti, offrendo maggiore flessibilità. Infine, il concetto di **dati strutturati vs non strutturati** distingue due grandi categorie di archiviazione: i **data warehouse** gestiscono dati strutturati, organizzati in modo rigido per l’analisi, mentre i **data lake** conservano dati grezzi e non strutturati, da elaborare successivamente a seconda delle necessità. La parte di elaborazione può essere suddivisa principalmente in due categorie: i **motori ottimizzati per la gestione delle transazioni e quelli progettati per l’analisi dei dati su larga scala**. Che si possono sintetizzare in **OLTP (Online Transaction Processing) e OLAP (Online Analytical Processing)**. I sistemi OLTP sono ottimizzati per operazioni frequenti su pochi record alla volta, tipici degli ambienti transazionali come l’elaborazione di pagamenti o la gestione degli ordini. Al contrario, i sistemi OLAP sono progettati per analizzare grandi volumi di dati e supportare operazioni complesse, come l’aggregazione e l’elaborazione di report, rendendoli ideali per il supporto alle decisioni e l’analisi avanzata.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ENyP8MsEiaI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training Data"
      ],
      "metadata": {
        "id": "WL1DAMu1KuYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Feature Engineering\n",
        "\n",
        "![feat_eng](https://drive.google.com/uc?id=1AwpVue_BJZRefmLDu2-ZuykNXcz_S3gR)\n",
        "\n",
        "\n",
        "La parte dedicata alla feature engineering è una parte interessante perché come spesso accade una delle difficoltà è cercare di **mappare dall'alto** questa fase di sviluppo che è parte integrante dello sviluppo di un servizio di machine learning. Spesso si fatica a **decidere dove posizionare** questa fase. In questo libro viene posizionato tra la parte di dati (data engineering e training data) e quella di model development. Su questa suddivisione sono particolarmente d'accordo anche se bisogna sempre ricordare che si tratta di un grafo di passaggi che può comportare delle reiterazioni del ciclo al fine di ottenere i risultati migliori.\n",
        "\n",
        "La mappatura prevede:\n",
        "1. **Gestione dei Valori Mancanti**: anche all'interno di questa categoria si affrontano differenze di valori mancanti che vengono mappati in relazione alla natura (non casualità MNAR, casualità condizionata MAR, completa casualità MCAR). Successivamente vengono affrontate le tecniche di eliminazione e di imputazione\n",
        "1. **Scaling**: qui la classica suddivisione tra normalizzazione e standardizzazione\n",
        "1. **Discretizzazione**: categorizzazione delle variabili sempre ricordando il trade-off che porta potenzialmente ai limiti nei punti di discontinuità delle classi e la perdita di granularità del dato.\n",
        "1. **Encoding variabili categoriche**: trasformazione numerica delle variabili categoriche, in questa sezione devo approfondire il \"hashing trick\", reso popolare dal pacchetto Vowpal Wabbit sviluppato da Microsoft.  \n",
        "1. **Feature Crossing**: tecnica per combinare due o più feature e generare nuove feature. Questa tecnica è utile per modellare le relazioni non lineari tra le variabili.\n",
        "1. **Embedding posizionali**: le trasformazioni tramite embedding, sia esso posizionale discreto o continuo tramite le \"fourier features\"-\n",
        "\n",
        "Tuttavia, la parte più interessante perché non ci ho mai riflettuto approfonditamente è il fenomeno definito come **\"Data Leakage\"**\n",
        "\n",
        "> Il data leakage si riferisce al fenomeno in cui una forma dell’etichetta “trapela” nel set di feature utilizzate per fare previsioni, mentre questa stessa informazione non è disponibile durante l’inferenza\n",
        "\n",
        "Con questo tema quindi ci si riferisce a tutte quelle volte in cui il target viene incorporato senza controllo nelle features e quindi nei modelli, portando di conseguenze pericolose in applicazione quando questo effetto era stato sottovalutato. Esempi negativi sono i modelli utilizzati durante il covid per prevedere le future evoluzioni, oppure nella competizione di Kaggle \"Ion Switching\" indetta dalla Università di Liverpool in cui i partecipanti erano riusciti a fare reverse engineering e ottenere le label del test. Sempre nell'amnbito di Kaggle come dimenticare **Pavel Pleskov**, che era stato sospeso proprio per lo stesso punto.\n",
        "\n",
        "**Non esiste un metodo infallibile** per evitare questo tipo di fuga di dati, ma si può mitigare il rischio tenendo traccia delle fonti dei dati e comprendendo come vengono raccolti e processati. Questo mi ricorda ancora quanto sia importante dedicare tempo all'analisi e all'approfondimento, meglio se congiuntamente con **esperti di dominio (SME)** per capire il dato che si sta analizzando. A questo fine tornano utili anche le attività di **feature importance e feature generalization**.\n",
        "\n",
        "> L’algoritmo esatto per misurare l’importanza delle feature è complesso, ma intuitivamente, l’importanza di una feature in un modello viene misurata in base a quanto peggiora la performance del modello se quella feature o un set di feature contenente quella feature viene rimosso"
      ],
      "metadata": {
        "id": "s4QzO3WEKu8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Model Development and Offline Evaluation\n",
        "\n",
        "**Sviluppo e Addestramento del Modello**\n",
        "\n",
        "> \"tutti i modelli sono sbagliati, ma alcuni sono utili\" George Box (1976)\n",
        "\n",
        "Siamo finalmente arrivati alla parte divertente per qualsiasi data scientist. Infatti, è difficile trovare spunti mai sentiti però è utile per ragionare sempre in ottica di framework. Infatti, partiamo dal punto che la valutazione dei modelli deve partire dalle **metriche di performance dipendenti dal target deciso (classificazione o regressione) tuttavia non bisogna tralasciare altre proprietà**: tra queste potrebbero esserci la quantità di dati, risorse computazionali, tempo per addestramento, latenza in ottica di inferenza e interpretabilità. Questo punto è coerente con l'approccio di tutto il libro che volontariamente tralascia gli aspetti più metodologici per mappare i sistemi di ML in modo omnicomprensivo. Interessante però che vengano sottolineati le conferenze più interessanti (NeurIPS, ICLR e ICML) o tecnici attivi sui social per rimanere aggiornati sulle metodologie stato dell'arte.\n",
        "Vengono proposti dall'autrice dei **consigli (5) per migliorare la selezione di un modello**. Viene consigliato di partire sempre da modelli più semplici per avere dei benchmark di valutazione senza complicare la parte di deployment in modo potenzialmente non necessario, ma anche valutare accuratamente i potenziali trade off come falsi negativi o positivi, e i limiti dovuti alle assunzioni dei modelli. Un aspetto che mi ha colpito e che faccio poco è provare a fare dei sample di dati e misurare la performance sia sulla totalità del campione sia su campioni distorti; questa parte viene riprodotta in parte dal classico split train-validation-test però occorre ricordarlo e potenzialmente estremizzare questo aspetto al fine di averne una misura effettiva della capacità predittiva del modello a condizioni variabili (campioni out of sample and out of time). Un aspetto umano che invece si consiglia di non trascurare è la possibilità di preferenze personali aprioristiche che potrebbero influenzare il numero di esperimenti con un dato modello (\"xgboost funziona sempre bene e quindi faccio tutti i test possibili di questo mentre tralascio metodologie che ritengo meno efficaci\").\n",
        "\n",
        "Empiricamente all'interno delle organizzazioni e soprattutto delle competizioni Kaggle e similari le **metodologie di \"ensemble\"** garantiscono performance migliori rispetto a modelli semplici, quindi è bene conoscerli in modo approfondito per applicarli in modo ottimale.\n",
        "Tecniche comuni:\n",
        "- **Bagging**: vengono creati dei sotto campioni con rimpiazzo (boostraps) e su questi vengono addestrati modelli separati che poi vengono aggregati (voto per maggioranza, media e altre metodologie)\n",
        "- **Boosting**: algoritmi iterativi che utilizzano sempre gli stessi dati ma i base learner vengono addestrati per ridurre gli errori dei precedenti e vengono pesati in modo decrescente\n",
        "- **Stacking**: quando è possibile risulta un'ottima soluzione stimare modelli diversi e se sufficientemente non correlati unirli in un passaggio successivo attraverso euristiche o un meta learner che potrebbe essere un modello aggiuntivo.\n",
        "\n",
        "La parte più interessante del capitolo riguarda la parte di experiment tracking e versioning. Il processo di monitoraggio dei progressi e dei risultati di un esperimento si chiama **tracciamento degli esperimenti**. Il processo di registrare tutti i dettagli di un esperimento, per poterlo eventualmente ricreare o confrontare con altri esperimenti, si chiama **versionamento**. Questa parte è interessante perché non esiste uniformità su come si possa strutturare questa parte. Ci sono degli strumenti che semplificano una delle due parti e per estensione poi fanno anche una parte della seconda ma non esiste univocità su come farlo e su cosa considerare come differenza tra due dati: basta una riga? o solo se il dato viene cambiato nella struttura o viene eliminato?\n",
        "\n",
        "In ogni caso occorre sempre ricordare che la complessità dei sistemi di ML si base sempre sulla necessità di tenere allineati tutti gli elementi coinvolti, che spesso afferiscono anche a team diversi.\n",
        "\n",
        "Il capitolo poi tratta dei temi che mi stanno poco a cuore per la distanza rispetto alle mie attività classica, ovvero il **data and model parallelism**. Queste sono metodologie, o forse le definirei più architetture, per risolvere problemi di dimensionalità dei dati nei casi in cui i dati e i modelli non possano essere gestiti nei limiti della memoria computazionale disponibile. Allo stesso modo anche la parte di **autoML** è un argomento interessante ma che andrebbe approfondito maggiormente per capire quali possano essere le applicazioni utili, ad esempio viene citata l'ottimizzazione degli iper parametri che è oggettivamente un'attività lunga e priva di valore aggiunto umano.\n",
        "\n",
        "Infine, vengono ricordati i **rischi connessi alla valutazione di un modello offline rispetto alla produzione**. Prima di eseguire il deploy di un modello è sempre bene testare dei modelli di riferimento (baseline) in modo da poter rispondere facilmente a domande che a volte si tralasciano per la loro potenziale semplicità. Quindi cosa dovremmo testare? In primis, modelli casuali e predizione generate da semplici euristiche come ad esempio predire sempre la classe più frequente; oppure, in caso sia disponibile è sempre interessante la performance di un modello di machine learning rispetto ad una valutazione umana, mentre se è disponibile una versione precedente del modello è obbligatorio confrontare le performance per capire se sono stati fatti effettivamente dei miglioramenti.\n",
        "Oltre ad ampliare il set di possibili predizioni contro cui testare i modelli è bene anche stressare questi modelli in modo da essere sicuri che i modelli siano robusti, fair e calibrati correttamente. In questo caso sono titoli interesanti per avere un framework di attività che spesso di fanno ma non vengono codificate in modo univoco:\n",
        "1. **perturbation test**: aggiungere rumore rispetto ai dati di training\n",
        "1. **inviariance test**: controllare che singole variabili non siano discriminatorie (es: razza)\n",
        "1. **directional expectation tests**: al crescere della dimensione degli immobili il valore complessivo non può mai diminuire\n",
        "1. **model calibration**: controllare l'effettiva calibrazione (questo è un passaggio da approfondire in quanto disponibile anche in scikit-learn)\n",
        "1. **confidence measurement**: cercare di dare delle misure di confidenze per potenzialmente filtrare predizioni su cui non siamo confidenti\n",
        "1. **slice based evaluation**: occorre ricordarsi sempre del Simpson's Paradox (fenomeno per cui i trend e le performance sul campione complessivo possano ribaltarsi completamente nei sottogruppi) e quindi testare sempre le performance in modo complessivo ma anche sui sottogruppi. I sottogruppi possono essere definiti per conoscenza di business, tramite analisi dell'errore e attraverso metodologie di scoperta automatica di sottogruppi sviluppate negli ultimi anni."
      ],
      "metadata": {
        "id": "yKlnfR7QWkIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Model Deployment and Prediction Service\n",
        "\n",
        "Concluso lo sviluppo del modello si parte con la fase di deployment dove il modello verrà utilizzato per fare inferenza. La responsabilità di questa fase dipende dalla struttura organizzativa che l'azienda ha deciso di assumere: potrebbe essere del data scientist autore dello sviluppo del modello oppure consegnato ad un team contiguo. La scelta è filosofica ma verrà approfondita meglio nel capitolo 11.\n",
        "\n",
        "Questa fase nasconde delle complessità che si potrebbero condensare in quelli che l'autrice definisce miti da sfatare:\n",
        "1. **Si Distribuiscono Solo Uno o Due Modelli ML alla Volta**  \n",
        "1. **Se Non Facciamo Nulla, le Prestazioni del Modello Rimangono Invariate**  \n",
        "1. **Non È Necessario Aggiornare Spesso i Modelli**\n",
        "1. **La Maggior Parte degli Ingegneri ML Non Deve Preoccuparsi della Scalabilità**  \n",
        "\n",
        "Semplificando quindi si potrebbe dire che i modelli devono essere manutenuti e aggiornati perché il contesto di applicazione e le relative performance potrebbero decadere, oppure potrebbe succedere solo a 1 dei 100 modelli che abbiamo in produzione. Ma una decisione fondamentale che influenzerà sia gli utenti finali che gli sviluppatori è il modo in cui il sistema genera e serve le previsioni: **online** o **batch**. Per evitare confusioni spesso però si usa la seguente suddivisione semantica:\n",
        "- **predizione sincrona (online)**: le richieste vengono inviate al servizio di previsione tramite API RESTful (es. richieste HTTP). Occorre quindi definire un payload ed esporre un endpoint tramite AWS Sagemaker o Google App Engine.\n",
        "- **predizione asincrona (batch)**: poiché le previsioni vengono generate in modo asincrono rispetto alle richieste\n",
        "\n",
        "Occorre ricordare che con hardware sempre più specializzati e potenti, e con lo sviluppo di tecniche più efficienti per rendere le predizioni online più veloci ed economiche, la predizione online potrebbe diventare lo standard. Riuscire ad unire l'online e il batch in modo coerente è una delle sfide attuali più dibattute. Feature store e strumenti come Apache Flink sembrano rendere possibile questa sfida. I data scientist spesso arrivano dal mondo della ricerca e si occupano di una specifica sezione dell'intero flusso. Quindi, occorre impegnarsi ad uscire dalla propria comfort zone per disegnare un flusso efficiente e manutenibile.\n",
        "\n",
        "![ml_workflow](https://drive.google.com/uc?id=1tfjNSt2OtvHvEuX4wiklGW96KdgLhxiv)\n",
        "\n",
        "Successivamente vengono trattati anche i temi di riduzione della dimensione dei modelli e dell'implementazione in cloud o on edge, ovvero all'interno dei device come gli smartphone per esempio. Tuttavia, sono tematiche che per il momento tendo a non affrontare nella mia realtà aziendale attuale."
      ],
      "metadata": {
        "id": "5TD94YvnELk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Data Distribution Shifts and Monitoring\n",
        "\n",
        "![monitoring](https://drive.google.com/uc?id=1tC1RC4FYcFe0ITREYJpaWB-9OUZPBq74)\n",
        "\n",
        "\n",
        "In un sistema di Machine Learning, il deployment del modello non rappresenta la fine del percorso, bensì l’inizio di una fase cruciale: quella del monitoraggio. Una volta messo in produzione, il modello è soggetto a una progressiva degradazione delle performance, dovuta a molteplici cause. Alcune sono comuni a qualsiasi sistema software — come problemi di deployment e dipendenze, crash o malfunzionamenti hardware — mentre altre sono tipiche dei modelli ML, come la presenza di edge cases, l’insorgere di feedback loop degenerativi o, più frequentemente, cambiamenti nella distribuzione dei dati in ingresso. Questo fenomeno, noto come **data distribution shift**, si verifica quando le caratteristiche dei dati in produzione differiscono da quelle osservate durante il training, portando a un peggioramento dell’accuratezza predittiva. I principali tipi di shift includono il *covariate shift* (cambiamenti nella distribuzione delle feature) e il *concept drift* (cambiamenti nella relazione tra input e output).\n",
        "\n",
        "Per gestire questi rischi, è essenziale dotarsi di un sistema di **monitoraggio e osservabilità** robusto. In ambito ML, questo significa monitorare non solo le metriche classiche di accuratezza, ma anche le distribuzioni di input, output, predizioni e feature. Strumenti come *Great Expectations* e *Deequ* supportano controlli su formati, intervalli e coerenza tra feature, mentre test statistici comunemente usati sono:\n",
        "- **Kolmogorov–Smirnov (K-S) test**: test a due campioni per confrontare distribuzioni univariate\n",
        "- **Least-Squares Density Difference**: stima la differenza tra due distribuzioni tramite il metodo dei minimi quadrati\n",
        "- **MMD (Maximum Mean Discrepancy)**: tecnica basata su kernel per confronti multivariati, incluso nella variante **Learned Kernel MMD**.\n",
        "  \n",
        "Inoltre, l'osservabilità, che si è affermata negli ultimi anni come concetto chiave, va oltre il semplice logging: consente di inferire lo stato interno del sistema attraverso la sua **telemetria**, ovvero gli output runtime (metriche di accuratezza, predizioni, features e input grezzi). In contesti complessi, come le architetture a microservizi, il *distributed tracing* (assegnazione di ID univoci agli eventi) permette di risalire con precisione alle cause degli errori. In definitiva, mentre l’**interpretabilità** ci aiuta a comprendere il comportamento del modello, l’**osservabilità** consente di mantenere sotto controllo l’intero ecosistema ML, rilevando anomalie e prevenendo il degrado delle performance.\n"
      ],
      "metadata": {
        "id": "BpjjGDJdXe3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 9. Continual Learning and Test in Production"
      ],
      "metadata": {
        "id": "3zsWbSybEMff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 10. Infrastructure and Tooling for MLOps\n",
        "\n",
        "![infrastructure](https://drive.google.com/uc?id=147yWgCiShtZoEv0eKABX9aZNrfMu1hMC)\n",
        "\n",
        "\n",
        "Nel mondo del Machine Learning, l’infrastruttura è essenziale per supportare lo sviluppo e l’esecuzione dei modelli. Lo **storage** può variare da semplici dischi locali (HDD o SSD) a soluzioni centralizzate come **Amazon S3 e Snowflake**, oppure essere distribuito tra più sedi, sia on-premise che nel cloud. Il **compute** rappresenta le risorse di calcolo disponibili, che possono spaziare da una singola CPU/GPU a infrastrutture cloud gestite come **AWS EC2 o GCP**. Framework come **Spark e Ray** elaborano i dati tramite **job**, mentre Kubernetes utilizza **pod** per gestire i container eseguibili. Due fattori chiave nella scelta dell’unità di calcolo sono la quantità di **memoria disponibile** e la **velocità di esecuzione** delle operazioni.  \n",
        "\n",
        "Il **cloud pubblico** offre flessibilità e scalabilità senza la necessità di gestire direttamente l’infrastruttura, risultando ideale per carichi di lavoro variabili come quelli del ML. Tuttavia, secondo un’analisi di **a16z**, il cloud può incidere pesantemente sui costi aziendali: per molte software company, l’infrastruttura cloud rappresenta circa il **50% dei costi di revenue**. Inoltre, le principali aziende software pubbliche avrebbero perso collettivamente circa **100 miliardi di dollari di valore di mercato** rispetto a un’infrastruttura gestita internamente, a causa dell’impatto del cloud sui margini.\n",
        "\n",
        "L’ambiente di sviluppo per il Machine Learning comprende gli IDE, gestione del versionamento e automatizzazione del deployment. Le aziende usano **Git** per il codice, **DVC** per i dati e strumenti come **Weights & Biases, Comet.ml e MLflow** per tracciare esperimenti e gestire i modelli. Per l’integrazione e la distribuzione continua (**CI/CD**), strumenti come **GitHub Actions e CircleCI** semplificano i workflow. I **notebook**, oltre a facilitare la scrittura di codice, integrano immagini e grafici per l'analisi esplorativa risultando essenziali per il lavoro dei data scientist. Per questo motivo, Netflix ha sviluppato soluzioni avanzate per potenziare l’uso dei notebook nel proprio ecosistema. Per garantire coerenza tra sviluppo e produzione, molte aziende standardizzano l’ambiente con **VS Code** o**Pycharm**. Mentre **GitHub Codespaces** o istanze cloud su **AWS EC2 e GCP** via SSH sono soluzioni ottime per scalare gli ambienti di sviluppo da locale a cloud.\n",
        "\n",
        "Il passaggio alla produzione è reso efficiente dai **container**, con **Docker** come soluzione principale per replicare ambienti tramite **Dockerfile**. In Docker, un’**image** rappresenta l’ambiente di esecuzione, mentre un **container** è un'istanza attiva di quell’immagine. Per gestire più container, si può usare **Docker Compose** su un singolo host o **Kubernetes (K8s)** per orchestrare deployment scalabili su più istanze.\n",
        "\n",
        "La gestione delle risorse in Machine Learning richiede strumenti per programmare ed eseguire i flussi di lavoro in modo efficiente. Gli **scheduler** stabiliscono **quando** eseguire un job e allocano le risorse necessarie, mentre gli **orchestratori** decidono **dove** ottenere tali risorse. Kubernetes è la soluzione più diffusa per orchestrare container su infrastrutture scalabili.  \n",
        "\n",
        "Nel contesto della Data Science, workflow complessi vengono gestiti con strumenti come **Airflow, Argo, Prefect, Kubeflow e Metaflow**, che organizzano le pipeline in **DAG** (grafi aciclici diretti). Questi workflow tipicamente includono fasi di **featurization, training e valutazione del modello** e possono essere definiti in **Python o YAML**. Ogni fase del processo corrisponde a un **task**, garantendo modularità e riproducibilità nelle operazioni di ML.\n",
        "\n",
        "Una **ML Platform** efficiente consente di gestire l’intero ciclo di vita dei modelli, dall’addestramento al deployment fino alla gestione delle feature. Tre elementi chiave in questo processo sono il **Model Deployment**, il **Model Store** e il **Feature Store**.  \n",
        "\n",
        "Il **Model Deployment** determina come un modello viene reso disponibile per le predizioni. Se le predizioni sono **online**, il modello viene esposto tramite un endpoint che elabora le richieste in tempo reale. Se invece si tratta di **predizioni batch**, il sistema recupera valori precomputati. Le principali soluzioni di deployment includono servizi cloud come **AWS SageMaker, GCP Vertex AI e Azure ML**, oltre a strumenti indipendenti come **MLflow Models, Seldon e Ray Serve**. La scelta della soluzione deve bilanciare scalabilità, facilità di gestione e supporto per entrambi i tipi di inferenza.  \n",
        "\n",
        "Un altro elemento cruciale è il **Model Store**, che va oltre il semplice salvataggio dei modelli. Strumenti come **MLflow** consentono di archiviare non solo il modello, ma anche informazioni fondamentali per il debugging e la riproducibilità, come parametri, dati di addestramento, codice, funzioni di featurization e artefatti sperimentali. Questo livello di tracciabilità è essenziale per garantire un deployment affidabile e aggiornamenti controllati dei modelli. Schematizzando i punti sono:\n",
        "  - Definizione del modello  \n",
        "  - Parametri  \n",
        "  - Funzioni di featurization e predizione  \n",
        "  - Dipendenze  \n",
        "  - Dati  \n",
        "  - Codice di generazione del modello  \n",
        "  - Artefatti degli esperimenti  \n",
        "  - Tag  \n",
        "\n",
        "Infine, il **Feature Store** risolve il problema della gestione e della coerenza delle feature tra training e inferenza. Un buon Feature Store supporta tre aspetti chiave: **Feature Management** (gestione delle feature), **Feature Transformation** (applicazione di trasformazioni) e **Feature Consistency** (coerenza tra le versioni dei dati). Strumenti come **Feast, Tecton, SageMaker Feature Store e Databricks Feature Store** funzionano come un **data warehouse per le feature**, permettendo di centralizzare la loro gestione e migliorare la qualità dei modelli.\n",
        "\n",
        "Rimane aperta l'eterna lotta di **Build vs Buy**, che consiste sostanzialmente nel costruire una ML Platform garantendosi più controllo ma richiede grandi investimenti, mentre acquistare soluzioni preconfezionate accelera il deployment ma limita la personalizzazione.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cwGbtTVMNBil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. The Human Side of Machine Learning\n",
        "\n",
        "Quando si integra il Machine Learning nell’**esperienza utente**, è fondamentale **garantire coerenza nelle predizioni**. Un sistema incoerente può confondere gli utenti e minare la fiducia nel modello. Ad esempio, se un assistente virtuale fornisce risposte diverse a domande simili o se un sistema di raccomandazione propone suggerimenti contraddittori, l’esperienza complessiva ne risente negativamente. Per mitigare questo problema, è utile adottare strategie di controllo della qualità e valutare l'affidabilità delle previsioni prima di presentarle agli utenti.  \n",
        "\n",
        "Un’altra sfida è legata alle **predizioni \"per lo più corrette\"**, ovvero risultati che, pur essendo in genere accurati, possono talvolta contenere errori significativi. Per affrontare questo problema, si ricorre spesso a un approccio *human-in-the-loop*, in cui gli esseri umani intervengono per selezionare o correggere le previsioni del modello. Inoltre, la **degradazione graduale delle prestazioni** (*smooth failing*) è un concetto chiave in contesti in cui velocità e accuratezza devono essere bilanciate. In alcuni scenari, un modello meno preciso ma più rapido può risultare preferibile rispetto a uno più accurato ma lento, specialmente quando la latenza è un fattore critico per l’usabilità.\n",
        "\n",
        "\n",
        "\n",
        "![e2edatascientist](https://drive.google.com/uc?id=1HDN6KrldIuDixWen4_de8yQ9iGW9Zq4J)\n",
        "\n",
        "**Struttura del Team**\n",
        "\n",
        "La collaborazione tra team cross-funzionali è essenziale per il successo dei sistemi di Machine Learning. Gli **esperti del dominio** (*Subject Matter Experts*, SME) non sono solo utenti finali, ma possono contribuire attivamente allo sviluppo del modello in diverse fasi del ciclo di vita: dalla formulazione del problema all’ingegneria delle caratteristiche, fino all’analisi degli errori e alla valutazione del modello. Il loro coinvolgimento è particolarmente prezioso anche nella fase di riordinamento delle predizioni e nella progettazione dell’interfaccia utente, affinché i risultati siano presentati in modo chiaro e interpretabile.  \n",
        "\n",
        "Un altro aspetto chiave nella struttura del team riguarda il **ruolo del data scientist** nella gestione del ciclo di vita del modello. Esistono due approcci principali: nel primo, il team di data science sviluppa i modelli, ma è un **team separato di ML engineering o piattaforma a occuparsi della messa in produzione**. Questo metodo, sebbene strutturato, può creare problemi di comunicazione, rendere più complesso il debugging e portare a un’inevitabile frammentazione delle responsabilità.\n",
        "\n",
        "Nel secondo approccio, invece, i **data scientist gestiscono l’intero processo**, diventando figure “full-stack” capaci di coprire sia lo sviluppo del modello sia la sua operatività in produzione. Questo garantisce maggiore autonomia, ma comporta anche il rischio di distogliere i data scientist dall’analisi dei dati per occuparsi di aspetti infrastrutturali, come la gestione di AWS, la scrittura di Dockerfile e il debugging di configurazioni. Tuttavia, aziende come Stitch Fix e Netflix sottolineano che il successo di questo approccio dipende fortemente dalla qualità degli strumenti messi a disposizione del team.\n",
        "\n"
      ],
      "metadata": {
        "id": "tY8y9tc9NCCL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80Ws-vIohu29"
      },
      "outputs": [],
      "source": []
    }
  ]
}