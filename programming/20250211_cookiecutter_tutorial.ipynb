{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMYl0EM7O4wPF3ZhJU7HVgo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PozzOver13/learning/blob/main/programming/20250211_cookiecutter_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![Cover](https://drive.google.com/uc?id=15BHOX67RYBd4JqEUUYqAcrJjtky7warb)\n"
      ],
      "metadata": {
        "id": "EpkDHJLJ4fwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "- https://cookiecutter-data-science.drivendata.org/#\n",
        "- https://drivendata.co/blog/ccds-v2\n",
        "- https://github.com/cookiecutter/cookiecutter\n",
        "- https://www.youtube.com/watch?v=dxUMBVTvbWw&t=53s\n",
        "- https://www.gnu.org/software/make/"
      ],
      "metadata": {
        "id": "GzMXfRUR7Hsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Cookiecutter 4 Data Science?\n",
        "\n",
        "    a logical, reasonably standardized but flexible project structure for data science\n",
        "\n",
        "La libreria **Cookiecutter Data Science** è un template progettato per strutturare in modo ordinato e riproducibile i progetti di data science. È basato su **Cookiecutter**, uno strumento Python per generare progetti da modelli predefiniti.\n",
        "\n",
        "### 🚀 **Caratteristiche principali**:\n",
        "- **Struttura chiara e modulare**: Organizza il codice, i dati e la documentazione in cartelle specifiche.\n",
        "- **Facile riproducibilità**: Favorisce l'uso di ambienti virtuali e versionamento del codice.\n",
        "- **Separazione tra dati grezzi e trasformati**: Minimizza il rischio di sovrascrittura accidentale dei dati originali.\n",
        "- **Pipeline standardizzata**: Promuove l'uso di script modulari e notebook Jupyter per l'analisi e il modello.\n",
        "\n",
        "### 📂 **Struttura tipica del progetto**\n",
        "Quando si usa il template, viene creata una struttura simile a questa:\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "├── LICENSE            <- Open-source license if one is chosen\n",
        "├── Makefile           <- Makefile with convenience commands like `make data` or `make train`\n",
        "├── README.md          <- The top-level README for developers using this project.\n",
        "├── data\n",
        "│   ├── external       <- Data from third party sources.\n",
        "│   ├── interim        <- Intermediate data that has been transformed.\n",
        "│   ├── processed      <- The final, canonical data sets for modeling.\n",
        "│   └── raw            <- The original, immutable data dump.\n",
        "│\n",
        "├── docs               <- A default mkdocs project; see www.mkdocs.org for details\n",
        "│\n",
        "├── models             <- Trained and serialized models, model predictions, or model summaries\n",
        "│\n",
        "├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n",
        "│                         the creator's initials, and a short `-` delimited description, e.g.\n",
        "│                         `1.0-jqp-initial-data-exploration`.\n",
        "│\n",
        "├── pyproject.toml     <- Project configuration file with package metadata for\n",
        "│                         {{ cookiecutter.module_name }} and configuration for tools like black\n",
        "│\n",
        "├── references         <- Data dictionaries, manuals, and all other explanatory materials.\n",
        "│\n",
        "├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n",
        "│   └── figures        <- Generated graphics and figures to be used in reporting\n",
        "│\n",
        "├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n",
        "│                         generated with `pip freeze > requirements.txt`\n",
        "│\n",
        "├── setup.cfg          <- Configuration file for flake8\n",
        "│\n",
        "└── {{ cookiecutter.module_name }}   <- Source code for use in this project.\n",
        "    │\n",
        "    ├── __init__.py             <- Makes {{ cookiecutter.module_name }} a Python module\n",
        "    │\n",
        "    ├── config.py               <- Store useful variables and configuration\n",
        "    │\n",
        "    ├── dataset.py              <- Scripts to download or generate data\n",
        "    │\n",
        "    ├── features.py             <- Code to create features for modeling\n",
        "    │\n",
        "    ├── modeling                \n",
        "    │   ├── __init__.py\n",
        "    │   ├── predict.py          <- Code to run model inference with trained models          \n",
        "    │   └── train.py            <- Code to train models\n",
        "    │\n",
        "    └── plots.py                <- Code to create visualizations   \n",
        "```\n",
        "\n",
        "\n",
        "### 🔧 **Installazione e utilizzo**\n",
        "1. **Installazione di Cookiecutter**:\n",
        "E' preferibile usare pipx install che installa il pacchetto in un ambiente virtuale isolato e lo rende disponibile a livello globale (quindi non nell'environment attivo).\n",
        "\n",
        "   ```bash\n",
        "   pipx install cookiecutter\n",
        "   ```\n",
        "2. **Creazione di un nuovo progetto con il template**:\n",
        "   ```bash\n",
        "   ccds https://github.com/drivendata/cookiecutter-data-science\n",
        "   ```\n",
        "3. **Seguire le istruzioni interattive** per personalizzare il progetto.\n",
        "\n",
        "### 🎯 **Vantaggi**\n",
        "✔️ Migliora la collaborazione e la manutenibilità  \n",
        "✔️ Evita il disordine nei file del progetto  \n",
        "✔️ Favorisce buone pratiche ingegneristiche  \n",
        "✔️ Semplifica l'integrazione in pipeline di produzione  "
      ],
      "metadata": {
        "id": "vswdfDK35LIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What's new in V2?\n",
        "\n",
        "Ci sono diverse novità entusiasmanti in **CCDS V2**. Queste includono:  \n",
        "\n",
        "- **Un nuovo entrypoint CLI, `ccds`**. Controllare il punto di accesso a riga di comando ci dà maggiore controllo sul processo di Cookiecutter, abilitando molte delle funzionalità descritte di seguito.  \n",
        "\n",
        "- **Maggiore flessibilità**: molte richieste di funzionalità riguardavano la possibilità di scegliere strumenti diversi rispetto ai valori predefiniti della V1 per svolgere un determinato compito. Man mano che gli strumenti evolvono e vengono adottati più ampiamente, vogliamo rendere più semplice per gli utenti utilizzare le migliori opzioni nel proprio template di progetto.  \n",
        "\n",
        "- **Documentazione migliorata** con più esempi, spiegazioni più chiare sulle scelte e sugli strumenti, e un aspetto più moderno. Trova la versione più recente su [cookiecutter-data-science.drivendata.org](https://cookiecutter-data-science.drivendata.org/) (la vecchia documentazione verrà presto reindirizzata qui).  \n",
        "\n",
        "- **Test per CCDS**: la V1 non includeva test. Ora eseguiamo test su diversi sistemi operativi per verificare effettivamente tutti i passaggi di Cookiecutter. Questo processo ha portato alla scoperta di errori nascosti nei vari OS, ma ora possiamo apportare modifiche future con maggiore sicurezza.  \n",
        "\n",
        "- **Una visione per l'estendibilità**: maggiore automazione, più contributi dalla community, più possibilità di personalizzazione e una collaborazione più efficace.  \n",
        "\n",
        "Abbiamo accennato al fatto che il template segue una filosofia simile a Unix: concatenare i migliori strumenti per ogni attività invece di cercare di essere una soluzione unica e monolitica. Vedremo nel dettaglio ogni attività per cui **CCDS V2** fornisce strumenti e analizzeremo come funzionano."
      ],
      "metadata": {
        "id": "0acLpx5s7W7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Website Documentation"
      ],
      "metadata": {
        "id": "ICxFrwOYdA7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why\n",
        "\n",
        "**Other people will thank you**\n",
        "\n",
        "> A well-defined, standard project structure means that a newcomer can begin to understand an analysis without digging in to extensive documentation. It also means that they don't necessarily have to read 100% of the code before knowing where to look for very specific things.\n",
        "\n",
        "**You will thank you**\n",
        "\n",
        "> A good project structure encourages practices that make it easier to come back to old work, for example separation of concerns, abstracting analysis as a DAG, and engineering best practices like version control.\n",
        "\n",
        "**Nothing here is binding**\n",
        "\n",
        "> \"Consistency within a project is more important. Consistency within one module or function is the most important. ... However, know when to be inconsistent -- sometimes style guide recommendations just aren't applicable. When in doubt, use your best judgment. Look at other examples and decide what looks best. And don't hesitate to ask!\"  PEP 8\n",
        "\n"
      ],
      "metadata": {
        "id": "o4resYIi82Vw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Opinions\n",
        "\n",
        "**Data analysis is a directed acyclic graph**\n",
        "\n",
        "Il modo migliore per garantire la riproducibilità è trattare la tua pipeline di analisi dei dati come un grafo aciclico diretto (DAG). Questo significa che ogni passaggio della tua analisi è un nodo in un grafo diretto senza cicli.\n",
        "\n",
        "**Raw data is immutable**\n",
        "\n",
        "- Scrivi codice che sposti i dati grezzi attraverso una pipeline fino alla tua analisi finale.  \n",
        "- Serializza o memorizza in cache gli output intermedi dei passaggi che richiedono molto tempo.  \n",
        "- Rendi possibile (e idealmente documentato e automatizzato) per chiunque riprodurre i tuoi prodotti finali con solo il codice in {{ cookiecutter.module_name }} e i dati in data/raw/ (e data/external/).  \n",
        "\n",
        "- Non modificare mai i tuoi dati grezzi, soprattutto non manualmente e non in Excel. Questo include cambiare formati di file o correggere errori che potrebbero interrompere uno strumento che sta tentando di leggere il tuo file di dati.  \n",
        "\n",
        "- Non sovrascrivere i tuoi dati grezzi con una versione elaborata o pulita.  \n",
        "- Non salvare più versioni dei dati grezzi.  \n",
        "\n",
        "**Data should (mostly) not be kept in source control**\n",
        "\n",
        "Se hai grandi quantità di dati, considera di archiviarli e sincronizzarli con un servizio cloud.\n",
        "\n",
        "**Tools for DAGs**\n",
        "\n",
        "Make è la scelta per cookiecutter.  \n",
        "Esistono altri strumenti per gestire i DAG scritti in Python, invece che in un linguaggio specifico. Tra i più popolari ci sono Airflow, Luigi, Snakemake, Prefect, Dagster e Joblib.\n",
        "\n",
        "**Notebooks are for exploration and communication, source files are for repetition**\n",
        "\n",
        "Notebooks = sviluppo ed esplorazione\n",
        "\n",
        "Source code = riproducibilità e standardizzazione\n",
        "\n",
        "**Refactor the good parts into source code**\n",
        "\n",
        "{{ cookiecutter.module_name }} è nativamente un python package quindi conviene utilizzarlo.\n",
        "\n",
        "**Keep your modeling organized**\n",
        "\n",
        "All'interno di /models per tenere traccia dei modelli stimati. Si può partire con file json per poi passare a soluzioni più evolute come MLflow.\n",
        "\n",
        "**Build from the environment up**\n",
        "\n",
        "Importanza dei package manager:\n",
        "- conda\n",
        "- virtualenv, virtualenvwrapper, Poetry, Pipenv e altri\n",
        "- docker per riproducibilità livello avanzato\n",
        "\n",
        "**Keep secrets and configuration out of version control**\n",
        "\n",
        "Crea un file `.env` nella cartella principale del progetto. Grazie al file `.gitignore`, questo file non verrà mai aggiunto al repository di controllo di versione.\n",
        "\n",
        "Utilizza un pacchetto chiamato `python-dotenv` per caricare tutte le voci di questo file come variabili d'ambiente, in modo che siano accessibili tramite `os.environ.get`.\n",
        "\n",
        "**AWS CLI configuration**\n",
        "\n",
        "Gestion credenziali tramite: ~/.aws/credentials\n",
        "\n",
        "**Encourage adaptation from a consistent default**\n",
        "\n",
        "1. Semplifica\n",
        "1. Espandi\n",
        "1. Riorganizza\n",
        "\n",
        "Nei progetti di lunga durata, la cartella notebooks può diventare congestionata. Un adattamento che abbiamo adottato è aggiungere una cartella di livello superiore chiamata research/ (e una corrispondente cartella data/research data) che contiene sottocartelle per i singoli esperimenti. Queste sottocartelle possono contenere i propri notebook, codice e persino i propri file Makefile, che ereditano dal Makefile del progetto principale."
      ],
      "metadata": {
        "id": "WwFVTASD_slJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the template\n",
        "\n",
        "**Project setup**\n",
        "\n",
        "**🚨 How to create a custom project?**\n",
        "\n",
        "Puoi passare un template personalizzato al comando `ccds` semplicemente specificandolo come primo argomento. Il template deve essere un percorso (locale o remoto) che punta al modello che vuoi utilizzare.\n",
        "\n",
        "**Esempio di utilizzo**\n",
        "\n",
        "1. **Usare un template remoto (GitHub):**\n",
        "   ```bash\n",
        "   ccds https://github.com/nomeutente/nome-template\n",
        "   ```\n",
        "   Questo comando scarica e utilizza il template specificato dall'URL.\n",
        "\n",
        "2. **Usare un template locale:**\n",
        "   Se hai un template salvato sul tuo computer, puoi passare il percorso della directory come argomento:\n",
        "   ```bash\n",
        "   ccds /percorso/al/template-locale\n",
        "   ```\n",
        "\n",
        "3. **Personalizzare un template con variabili definite**:\n",
        "   Se il template richiede dei valori (ad esempio, nome del progetto, autore, ecc.), `ccds` ti chiederà di inserirli manualmente durante l'esecuzione.  \n",
        "   Oppure puoi passare le variabili in un file `cookiecutter.json` (nella stessa directory del template) o utilizzando il comando interattivo.\n",
        "\n",
        "**Note importanti**\n",
        "- **Struttura del template**: il tuo template deve avere una directory con un file `cookiecutter.json` contenente le variabili che vuoi personalizzare.\n",
        "- **Pre-requisiti**: assicurati di avere `cookiecutter` installato correttamente nel tuo ambiente se usi template personalizzati.\n",
        "- **Cambiare alberatura**: Se vuoi personalizzare dinamicamente la struttura delle directory (es. aggiungere o rimuovere cartelle in base a certe opzioni), puoi farlo utilizzando un hook script. I hook sono script Python o shell che vengono eseguiti prima o dopo la generazione del progetto, e possono modificare la struttura in base ai parametri forniti.\n",
        "\n",
        "**Set up version control**\n",
        "\n",
        "**🚨 How to create a repository on gitlab from local?**\n",
        "\n",
        "    Fare una prova da repository già inizializzata\n",
        "\n",
        "\n",
        "**Make as a task runner**\n",
        "\n",
        "    Installare make su windows\n",
        "\n",
        "**Create a Python virtual environment**\n",
        "\n",
        "    Usare make commands pre configurati (Conda env + install requirements)\n",
        "\n",
        "**Add your data**\n",
        "\n",
        "1. data/raw make sync_data_up -> s3\n",
        "1. data/make_dataset.py -> data/raw\n",
        "1. .env + data/db.py -> data/raw\n",
        "\n",
        "**Check out a branch**\n",
        "\n",
        "    Usa i branch\n",
        "\n",
        "**Notebook & Naming convention**\n",
        "\n",
        "Now you're ready to do some analysis! Make sure that your project-specific environment is activated (you can check with which jupyter) and run jupyter notebook notebooks to open a Jupyter notebook in the notebooks/ folder. You can start by creating a new notebook and doing some exploratory data analysis. We often name notebooks with a scheme that looks like this:\n",
        "\n",
        "    0.01-pjb-data-source-1.ipynb\n",
        "\n",
        "* 0.01 - Helps leep work in chronological order. The structure is PHASE.NOTEBOOK. NOTEBOOK is just the Nth notebook in that phase to be created. For phases of the project, we generally use a scheme like the following, but you are welcome to design your own conventions:\n",
        "  * 0 - Data exploration - often just for exploratory work\n",
        "  * 1 - Data cleaning and feature creation - often writes data to data/processed or data/interim\n",
        "  * 2 - Visualizations - often writes publication-ready viz to reports\n",
        "  * 3 - Modeling - training machine learning models\n",
        "  * 4 - Publication - Notebooks that get turned directly into reports\n",
        "* pjb - Your initials; this is helpful for knowing who created the notebook and prevents collisions from people working in the same notebook.\n",
        "* data-source-1 - A description of what the notebook covers\n",
        "Now that you have your notebook going, start your analysis!\n",
        "\n",
        "**Refactoring code into shared modules**\n",
        "\n",
        "    %load_ext autoreload\n",
        "    %autoreload 2\n",
        "\n",
        "**Make your code reviewable**\n",
        "\n",
        "    nbautoexport per superare il fatto che i notebook non sono consultabili su github o gitlab facilmente per una review\n",
        "\n",
        "**Changing the Makefile**\n",
        "\n",
        "    Consigliano di usarli per anche altre soluzioni\n",
        "\n",
        "**Installing Make on Windows**\n",
        "\n",
        "1. chocolatey\n",
        "1. winget\n",
        "1. scoop"
      ],
      "metadata": {
        "id": "2Re9CyZe9yAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make, Airflow & AWS\n",
        "\n",
        "### **GNU Make e Airflow sono strumenti alternativi?**\n",
        "Non sono strumenti completamente alternativi, ma possono sovrapporsi in alcune funzionalità, a seconda di come li utilizzi:\n",
        "\n",
        "1. **GNU Make**  \n",
        "   - È uno strumento per automatizzare attività basate su regole di dipendenza. Si concentra su **compilazione di codice**, ma è molto flessibile e può essere usato per altre automazioni, come pipelines di analisi o ETL.  \n",
        "   - **Punto di forza**: Semplice, funziona bene per progetti locali o in ambienti limitati.  \n",
        "   - **Limiti**: Non è progettato per orchestrare processi distribuiti o complessi in ambienti moderni.\n",
        "\n",
        "2. **Apache Airflow**  \n",
        "   - È una piattaforma avanzata per orchestrare **DAG (Directed Acyclic Graph)**, usata per automazione e pianificazione di workflow complessi, come pipeline ETL e machine learning.  \n",
        "   - **Punto di forza**: Gestione distribuita, monitoraggio tramite UI e integrazione con molte piattaforme.  \n",
        "   - **Limiti**: Overhead maggiore rispetto a GNU Make, più adatto a processi su larga scala.\n",
        "\n",
        "Quindi, mentre GNU Make e Airflow possono essere usati per automatizzare pipeline, **Airflow è progettato per ambienti distribuiti e più complessi**, mentre Make si presta meglio a progetti più semplici o specifici.\n",
        "\n",
        "---\n",
        "\n",
        "### **Esiste qualcosa di paragonabile in AWS?**\n",
        "AWS offre diversi strumenti per orchestrare workflow complessi e automatizzare pipeline, paragonabili ad Airflow (e in parte anche a GNU Make):\n",
        "\n",
        "1. **AWS Step Functions**  \n",
        "   - **Descrizione**: Strumento serverless per orchestrare workflow tramite stati definiti. Può gestire processi complessi con step sequenziali o paralleli.  \n",
        "   - **Paragone**:\n",
        "     - Simile ad Airflow in quanto supporta DAG e processi distribuiti.\n",
        "     - Differisce da GNU Make poiché si basa su JSON/State Machine piuttosto che su file Makefile.\n",
        "   - **Vantaggi**: Integrazione nativa con altri servizi AWS come Lambda, S3, DynamoDB, ecc.\n",
        "\n",
        "2. **AWS Glue Workflow**  \n",
        "   - **Descrizione**: Uno strumento per orchestrare processi ETL, integrato con Glue per la trasformazione dei dati. È pensato specificamente per pipeline di dati.  \n",
        "   - **Paragone**:\n",
        "     - Più simile ad Airflow, ma focalizzato su dati e ETL.\n",
        "     - Non è generico come Make.\n",
        "\n",
        "3. **Amazon Managed Workflows for Apache Airflow (MWAA)**  \n",
        "   - **Descrizione**: Un servizio completamente gestito di AWS per eseguire Apache Airflow.  \n",
        "   - **Paragone**:\n",
        "     - Offre tutte le funzionalità di Airflow, ma in un ambiente AWS gestito.\n",
        "     - Può essere una scelta naturale se già utilizzi Airflow e lavori su AWS.\n",
        "\n",
        "4. **Amazon EventBridge**  \n",
        "   - **Descrizione**: Uno strumento per gestire eventi e attivare processi automatizzati basati su regole (ad esempio, scatenare script Python quando un file è caricato su S3).  \n",
        "   - **Paragone**:\n",
        "     - Meno potente di Airflow o Step Functions per orchestrazioni complesse, ma molto utile per automazioni più semplici.\n",
        "\n",
        "---\n",
        "\n",
        "### **Quando usare cosa**\n",
        "- **GNU Make**: Per progetti locali o semplici task di automazione, soprattutto in ambito sviluppo.\n",
        "- **Airflow**: Per orchestrare workflow complessi o distribuiti, soprattutto in ambito dati o machine learning.\n",
        "- **AWS Step Functions**: Per workflow generici, distribuiti e serverless su AWS.\n",
        "- **AWS Glue Workflow**: Per pipeline ETL su AWS.\n",
        "- **Amazon MWAA**: Se hai bisogno di Airflow, ma preferisci un setup gestito su AWS.\n",
        "\n",
        "Se il tuo progetto è già su AWS e necessita di orchestrare workflow su più servizi, **Step Functions** o **Glue Workflow** potrebbero essere le opzioni migliori."
      ],
      "metadata": {
        "id": "uh6SyuGXW_FO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MDZBnte89w2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxTQQiSZ4a8m"
      },
      "outputs": [],
      "source": []
    }
  ]
}