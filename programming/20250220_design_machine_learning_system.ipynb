{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdYubbX9EPR7lTAAyxK9Mk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PozzOver13/learning/blob/main/programming/20250220_design_machine_learning_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Design Machine Learning Systems by Chip Huyen\n",
        "\n",
        "# Chip Huyen\n",
        "\n",
        "Chip Huyen è una **scrittrice e computer scientist specializzata in machine learning e MLOps**. Ha lavorato presso NVIDIA e Snorkel AI e ha co-fondato Claypot AI, una startup focalizzata sull’implementazione di modelli di machine learning in tempo reale. Inoltre, ha **insegnato alla Stanford University** un corso su MLOps e sistemi di ML.\n",
        "\n",
        "Chip Huyen è nota per i suoi post e articoli su blog e social, dove condivide riflessioni su AI, MLOps e ingegneria del software applicata al machine learning."
      ],
      "metadata": {
        "id": "F8iyi8Egh6eP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Overview of Machine Learning Systems\n",
        "\n",
        "![ml_system](https://drive.google.com/uc?id=1kSeTKfKpGuR_P2pV58MUkbn9DlJ8frGy)\n",
        "\n",
        "- L'algoritmo è solo una piccola parte di un sistema di Machine Learning in produzione. Il sistema comprende anche i requisiti di business che hanno dato origine al progetto di ML, l'interfaccia attraverso cui utenti e sviluppatori interagiscono con il sistema, lo stack dati e la logica per sviluppare, monitorare e aggiornare i modelli, oltre all'infrastruttura che consente l'erogazione di questa logica.\n",
        "\n",
        "- L'obiettivo di questo libro è fornirti un **framework per sviluppare una soluzione** che si adatti al meglio al tuo problema, indipendentemente dall'algoritmo che finirai per utilizzare.\n",
        "\n",
        "**Different stakeholders and requirements**\n",
        "\n",
        "- Sono molti gli stakeholder coinvolti nel portare un sistema di ML in produzione, ciascuno con i propri requisiti. La presenza di requisiti diversi, spesso in conflitto tra loro, può rendere difficile progettare, sviluppare e selezionare un modello di ML che li soddisfi tutti.\n",
        "\n",
        "**Computational priorities**\n",
        "\n",
        "- Quando si progetta un sistema di ML, chi non ha mai implementato un sistema del genere spesso commette l'errore di concentrarsi troppo sulla fase di sviluppo del modello, trascurando invece il deployment e la manutenzione.\n",
        "\n",
        "- Il termine \"latenza\" si riferisce al tempo di risposta, quindi la latenza di una richiesta misura il tempo che intercorre tra l'invio della richiesta e la ricezione della risposta. È una pratica comune utilizzare percentili elevati per specificare i requisiti di performance di un sistema; ad esempio, un product manager potrebbe stabilire che la latenza del 90° percentile o del 99,9° percentile debba rimanere al di sotto di una determinata soglia.\n",
        "\n",
        "**Fairness**\n",
        "> Gli algoritmi di ML non prevedono il futuro, ma codificano il passato, perpetuando così i bias presenti nei dati e oltre. Quando vengono implementati su larga scala, gli algoritmi di ML possono discriminare le persone su larga scala.\n",
        "\n",
        "**Interpretability**\n",
        "- All'inizio del 2020, il vincitore del Turing Award, il professor Geoffrey Hinton, sollevò una questione molto dibattuta sull'importanza dell'interpretabilità nei sistemi di ML. “Supponiamo che tu abbia un cancro e debba scegliere tra un chirurgo AI di tipo black-box, che non può spiegare come funziona ma ha un tasso di guarigione del 90%, e un chirurgo umano con un tasso di guarigione dell'80%. Vuoi che il chirurgo AI sia illegale?”\n",
        "\n",
        "- Innanzitutto, l'interpretabilità è importante affinché gli utenti, sia i leader aziendali che gli utenti finali, comprendano il motivo di una decisione, così da poter fidarsi di un modello e individuare eventuali bias. In secondo luogo, è essenziale per gli sviluppatori, affinché possano eseguire il debug e migliorare il modello.\n",
        "\n",
        "**Machine Learning Systems Versus Traditional Software**\n",
        "- I sistemi di ML sono in parte codice, in parte dati e in parte artefatti generati dall'unione dei due. La tendenza dell'ultimo decennio mostra che le applicazioni sviluppate con più dati, e dati migliori, prevalgono. Invece di concentrarsi sul miglioramento degli algoritmi di ML, la maggior parte delle aziende si concentrerà sul miglioramento dei dati. Poiché i dati possono cambiare rapidamente, le applicazioni di ML devono adattarsi a un ambiente in continua evoluzione, il che può richiedere cicli di sviluppo e deployment più rapidi.\n",
        "\n"
      ],
      "metadata": {
        "id": "LL7uX1Cb0qhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Introduction to Machine Learning Systems Design\n",
        "\n",
        "**Obiettivi di Business e ML**  \n",
        "- Quali metriche interessano alle aziende? Anche se molte aziende vogliono convincerti del contrario, secondo l’economista premio Nobel Milton Friedman, l’unico scopo delle imprese è massimizzare i profitti per gli azionisti.  \n",
        "- I ritorni sugli investimenti in ML dipendono molto dal livello di maturità nell'adozione. Più a lungo un'azienda ha adottato il ML, più efficiente sarà la pipeline, più rapido sarà il ciclo di sviluppo, meno tempo di ingegneria sarà necessario e più bassi saranno i costi del cloud, il che porta a ritorni più elevati.  \n",
        "\n",
        "**Requisiti per i Sistemi di ML**  \n",
        "Quattro requisiti fondamentali: **affidabilità, scalabilità, manutenibilità e adattabilità**.  \n",
        "\n",
        "- **Affidabilità**  \n",
        "  - Il sistema dovrebbe continuare a svolgere la funzione corretta al livello di prestazioni desiderato anche in condizioni avverse (guasti hardware/software o errori umani). La \"correttezza\" può essere difficile da definire nei sistemi ML.  \n",
        "\n",
        "- **Scalabilità**  \n",
        "  - Un sistema di ML può crescere in diversi modi:  \n",
        "    - Può aumentare in complessità.  \n",
        "    - Può gestire un volume di traffico maggiore.  \n",
        "    - Può supportare un numero crescente di modelli di ML.  \n",
        "  - Un elemento indispensabile nei servizi cloud è **l'auto-scalabilità**, ovvero la capacità di aumentare o diminuire automaticamente il numero di macchine in base all'uso. Tuttavia, questa funzionalità può essere complessa da implementare.  \n",
        "\n",
        "- **Manutenibilità**  \n",
        "  - È fondamentale strutturare i flussi di lavoro e l’infrastruttura in modo che i vari team possano usare strumenti con cui si sentono a proprio agio, evitando che un gruppo imponga le proprie scelte agli altri.  \n",
        "  - Il codice dovrebbe essere documentato.  \n",
        "  - Codice, dati e artefatti dovrebbero essere versionati.  \n",
        "  - I modelli dovrebbero essere riproducibili, in modo che chiunque possa lavorarci anche se gli autori originali non sono disponibili.  \n",
        "  - Quando si verifica un problema, i vari team dovrebbero collaborare per risolverlo senza attribuire colpe.  \n",
        "\n",
        "- **Adattabilità**  \n",
        "  - I sistemi di ML devono evolversi rapidamente per rispondere ai cambiamenti del contesto.  \n",
        "\n",
        "**Processo Iterativo**  \n",
        "![ml_lifecycle](https://drive.google.com/uc?id=1yLrbgsCN91BaHf6nlVeuSvwMmHEfY34D)\n",
        "\n",
        "- Sviluppare un sistema ML è un processo iterativo e, nella maggior parte dei casi, senza una vera fine. Una volta che un sistema è in produzione, deve essere monitorato e aggiornato continuamente.  \n",
        "\n",
        "**Definizione dei Problemi di ML**  \n",
        "- Un’assistenza clienti lenta è un problema, ma non è un problema di ML. Un problema di ML è definito da **input, output e funzione obiettivo**, che guidano il processo di apprendimento. Nessuna di queste tre componenti è ovvia da una semplice richiesta del capo. È compito dell'ingegnere ML tradurre il problema in un caso applicabile al ML.  \n",
        "\n",
        "**Tipi di Task di ML**  \n",
        "- L’output del modello determina la tipologia del problema ML. Le due categorie principali sono **classificazione e regressione**.  \n",
        "\n",
        "**Decoupling degli Obiettivi**  \n",
        "- Quando ci sono più obiettivi, è una buona pratica separarli, perché:  \n",
        "  1. Rende più semplice lo sviluppo e la manutenzione del modello.  \n",
        "  2. Permette di modificare il sistema senza dover riaddestrare i modelli.  \n",
        "  3. Facilita la manutenzione, poiché obiettivi diversi possono avere esigenze di aggiornamento diverse.  \n",
        "\n",
        "**Mente vs Dati**  \n",
        "- Nel dibattito tra mente e dati, il Dr. **Judea Pearl**, vincitore del Turing Award e noto per i suoi studi sull'inferenza causale e le reti bayesiane, ha affermato:  \n",
        "  > \"I dati sono profondamente stupidi.\"  \n",
        "\n",
        "- **Peter Norvig**, direttore della qualità della ricerca di Google, ha enfatizzato il ruolo dei dati rispetto agli algoritmi affermando:  \n",
        "  > \"Non abbiamo algoritmi migliori. Abbiamo solo più dati.\"  \n",
        "\n",
        "- Se vuoi usare la data science (di cui il ML è una branca) per migliorare prodotti o processi, devi partire **dalla qualità e dalla quantità dei dati**.  \n",
        "- **Senza dati, non esiste data science.**"
      ],
      "metadata": {
        "id": "AiunTjJe0xb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Engineering Fundamentals\n",
        "\n",
        "![etl_process](https://drive.google.com/uc?id=1fUfU5Ap5qmNZl5t2JZYabnGVmgswAwfw)\n",
        "\n",
        "\n",
        "- Se vuoi approfondire l'ingegneria dei dati da una prospettiva sistemistica, ti consiglio l'eccellente libro di Martin Kleppmann *Designing Data-Intensive Applications* (O’Reilly, 2017).  \n",
        "\n",
        "**Fonti di Dati**  \n",
        "- I dati di **prima parte** sono quelli che la tua azienda raccoglie direttamente sugli utenti o clienti.  \n",
        "- I dati di **seconda parte** sono raccolti da un'altra azienda sui propri clienti e messi a disposizione (spesso a pagamento).  \n",
        "- I dati di **terza parte** sono raccolti da aziende che monitorano il pubblico senza avere una relazione diretta con esso.  \n",
        "\n",
        "**Formati di Dati**  \n",
        "- Il processo di conversione di una struttura dati o dello stato di un oggetto in un formato archiviabile o trasmissibile, ricostruibile in seguito, è chiamato **serializzazione dei dati**. Esistono moltissimi formati di serializzazione, e nella scelta bisogna considerare leggibilità umana, pattern di accesso, e se il formato è testuale o binario (impattando sulle dimensioni dei file).  \n",
        "- Wikipedia ha un'ottima pagina intitolata *Comparison of Data-Serialization Formats*.  \n",
        "\n",
        "**JSON**  \n",
        "- JSON (*JavaScript Object Notation*) è onnipresente. Pur derivando da JavaScript, è indipendente dal linguaggio: la maggior parte dei linguaggi moderni può generarlo e analizzarlo. È leggibile dall’uomo e utilizza un paradigma chiave-valore potente e adatto a dati con diversi livelli di struttura.  \n",
        "\n",
        "**Formato Row-Major vs Column-Major**  \n",
        "- Due formati comuni, che rappresentano due paradigmi distinti, sono **CSV** e **Parquet**.  \n",
        "  - **CSV** è **row-major**, ossia gli elementi consecutivi di una riga sono memorizzati insieme in memoria.  \n",
        "  - **Parquet** è **column-major**, ossia gli elementi consecutivi di una colonna sono memorizzati insieme.  \n",
        "- I formati **row-major** sono migliori per scritture frequenti, mentre quelli **column-major** sono più efficienti nelle letture colonnari.  \n",
        "- Accedere a un DataFrame per riga è molto più lento che accedervi per colonna. Tuttavia, convertendo lo stesso DataFrame in un `ndarray` di NumPy, l’accesso per riga diventa molto più veloce.  \n",
        "\n",
        "**Formato Testuale vs Binario**  \n",
        "- **CSV e JSON** sono file di **testo**, mentre **Parquet** è un file **binario**.  \n",
        "- I file testuali sono leggibili dall’uomo, mentre i file binari sono destinati a essere interpretati da programmi che leggono byte grezzi.  \n",
        "- **AWS consiglia il formato Parquet** perché permette uno scaricamento fino a **2 volte più veloce** e consuma fino a **6 volte meno spazio** rispetto ai formati testuali su Amazon S3.  \n",
        "\n",
        "**Modelli di Dati**  \n",
        "**Modello Relazionale**  \n",
        "- I dati sono organizzati in **relazioni** (insiemi di tuple). Una tabella è la rappresentazione visiva di una relazione, e ogni riga è una tupla.  \n",
        "- La **normalizzazione** sparge i dati tra più relazioni: questo riduce la ridondanza ma rende costose le operazioni di join su tabelle grandi.  \n",
        "- **SQL è un linguaggio dichiarativo**, a differenza di Python che è imperativo.  \n",
        "  - In un linguaggio **imperativo**, si specificano i passaggi per ottenere un risultato.  \n",
        "  - In un linguaggio **dichiarativo**, si specifica solo il risultato desiderato e il sistema trova i passaggi necessari.  \n",
        "- Esempi di framework ML dichiarativi: **Ludwig** (Uber) e **H2O AutoML**.  \n",
        "\n",
        "**NoSQL**  \n",
        "- Inizialmente il termine NoSQL era un hashtag per incontri sui database non relazionali, poi è stato reinterpretato come *Not Only SQL*.  \n",
        "- I due principali modelli NoSQL sono:  \n",
        "  - **Modello a documenti**, utile quando i dati sono contenuti in documenti indipendenti con relazioni rare.  \n",
        "  - **Modello a grafo**, utile quando le relazioni tra elementi sono comuni e importanti.  \n",
        "\n",
        "**Modello a Documenti**  \n",
        "- Nei database a documenti, la struttura viene determinata dal sistema che legge i dati anziché da quello che li scrive.  \n",
        "- Molti database, come **PostgreSQL e MySQL**, supportano sia il modello relazionale che quello a documenti.  \n",
        "\n",
        "**Modello a Grafo**  \n",
        "- Usato per dati con molte relazioni complesse e connesse.  \n",
        "\n",
        "**Dati Strutturati vs Non Strutturati**  \n",
        "- Un **data warehouse** memorizza dati strutturati.  \n",
        "- Un **data lake** conserva dati non strutturati (spesso grezzi, da processare successivamente).  \n",
        "\n",
        "**Motori di Archiviazione e Elaborazione**  \n",
        "**Elaborazione Transazionale vs Analitica**  \n",
        "- **Database transazionali** sono progettati per gestire transazioni con **bassa latenza e alta disponibilità**.  \n",
        "- Quando si parla di database transazionali, si pensa subito alle proprietà **ACID** (Atomicità, Consistenza, Isolamento, Durabilità).  \n",
        "- **OLTP vs OLAP**  \n",
        "  - **OLTP** (Online Transaction Processing): ottimizzato per operazioni rapide e frequenti su pochi record.  \n",
        "  - **OLAP** (Online Analytical Processing): ottimizzato per analisi su grandi quantità di dati.  \n",
        "\n",
        "**ETL: Extract, Transform, Load**  \n",
        "- ETL è il processo generale di estrazione, trasformazione e caricamento dei dati nel formato desiderato.  \n",
        "\n",
        "**Modalità di Flusso dei Dati**  \n",
        "**Dati che Passano Attraverso Database**  \n",
        "- Entrambi i processi accedono ai dati via database, ma letture/scritture possono essere lente e inadatte per applicazioni con requisiti di latenza stretti (come le app consumer).  \n",
        "\n",
        "**Dati che Passano Attraverso Servizi**  \n",
        "- Due principali stili di richiesta:  \n",
        "  - **REST** (*Representational State Transfer*): progettato per richieste su rete.  \n",
        "  - **RPC** (*Remote Procedure Call*): cerca di rendere una richiesta di rete simile a una chiamata a funzione locale.  \n",
        "- **REST è dominante nelle API pubbliche**, mentre **RPC** è più usato per richieste interne tra servizi della stessa organizzazione.  \n",
        "\n",
        "**Dati che Passano Attraverso Trasporto in Tempo Reale**  \n",
        "- Invece di usare database come intermediari, si usa la **memoria**.  \n",
        "- Il **trasporto in tempo reale** può essere visto come una memoria per lo scambio di dati tra servizi.  \n",
        "- Un dato trasmesso nel sistema si chiama **evento**, per questo l’architettura è anche detta **event-driven**.  \n",
        "- Un sistema di trasporto in tempo reale è chiamato **event bus**.  \n",
        "\n",
        "**Elaborazione in Batch vs Stream Processing**  \n",
        "- **Batch processing**: l’elaborazione avviene su dati accumulati in blocchi.  \n",
        "  - Sistemi come **MapReduce e Spark** sono progettati per elaborare dati batch in modo efficiente.  \n",
        "- **Stream processing**: i dati vengono elaborati in tempo reale mentre fluiscono attraverso sistemi come **Apache Kafka e Amazon Kinesis**.  \n",
        "  - Può essere eseguito periodicamente, ma con intervalli più brevi rispetto al batch processing.  "
      ],
      "metadata": {
        "id": "ENyP8MsEiaI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 4. Training Data\n",
        "## 5. Feature Engineering\n",
        "## 6. Model Development and Offline Evaluation\n",
        "\n"
      ],
      "metadata": {
        "id": "yKlnfR7QWkIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Model Deployment and Prediction Service\n",
        "\n",
        "**Introduzione**  \n",
        "- In molte aziende, la responsabilità di distribuire i modelli ricade sugli stessi sviluppatori che li hanno creati. In molte altre, una volta che un modello è pronto per il deployment, viene esportato e consegnato a un altro team per l’implementazione. Tuttavia, questa separazione di responsabilità può causare un’elevata comunicazione tra i team e rallentare l’aggiornamento del modello.  \n",
        "- Il processo di generazione delle previsioni è chiamato **inferenza**.  \n",
        "\n",
        "**Miti sul Deployment dei Modelli di Machine Learning**  \n",
        "**Mito 1: Si Distribuiscono Solo Uno o Due Modelli ML alla Volta**  \n",
        "- In realtà, Uber ha migliaia di modelli in produzione. In qualsiasi momento, Google ha migliaia di modelli in fase di addestramento, alcuni con centinaia di miliardi di parametri. Booking.com utilizza oltre 150 modelli. Uno studio del 2021 di Algorithmia mostra che tra le organizzazioni con più di 25.000 dipendenti, il 41% ha più di 100 modelli in produzione.  \n",
        "\n",
        "**Mito 2: Se Non Facciamo Nulla, le Prestazioni del Modello Rimangono Invariate**  \n",
        "- I sistemi di machine learning soffrono di quelli che vengono chiamati **cambiamenti nella distribuzione dei dati**, ovvero situazioni in cui la distribuzione dei dati incontrata in produzione è diversa da quella su cui il modello è stato addestrato.  \n",
        "\n",
        "**Mito 3: Non È Necessario Aggiornare Spesso i Modelli**  \n",
        "- Poiché le prestazioni di un modello decadono nel tempo, è importante aggiornarlo il più rapidamente possibile. Questo è un aspetto del ML in cui possiamo imparare dalle migliori pratiche del DevOps. Già nel 2015, aziende come Etsy rilasciavano aggiornamenti 50 volte al giorno, Netflix migliaia di volte al giorno e AWS ogni 11,7 secondi.  \n",
        "\n",
        "**Mito 4: La Maggior Parte degli Ingegneri ML Non Deve Preoccuparsi della Scalabilità**  \n",
        "- (Nessun contenuto fornito nel testo originale).  \n",
        "\n",
        "**Predizione in Batch vs. Predizione Online**  \n",
        "- Una decisione fondamentale che influenzerà sia gli utenti finali che gli sviluppatori è il modo in cui il sistema genera e serve le previsioni: **online** o **batch**.  \n",
        "- Tradizionalmente, nella predizione online, le richieste vengono inviate al servizio di previsione tramite API RESTful (es. richieste HTTP). Quando le richieste di previsione vengono inviate via HTTP, si parla anche di **predizione sincrona**, poiché le previsioni vengono generate in tempo reale rispetto alle richieste.  \n",
        "- La **predizione batch** è anche nota come **predizione asincrona**, poiché le previsioni vengono generate in modo asincrono rispetto alle richieste.  \n",
        "- I termini \"predizione online\" e \"predizione batch\" possono creare confusione, perché entrambi possono elaborare più campioni alla volta (batch) o un campione per volta. Per evitare questa ambiguità, a volte si preferiscono i termini **\"predizione sincrona\"** e **\"predizione asincrona\"**.  \n",
        "\n",
        "**Dalla Predizione Batch alla Predizione Online**  \n",
        "- Si esporta il modello, lo si carica su Amazon SageMaker o Google App Engine e si ottiene un endpoint esposto. Ora, se si invia una richiesta contenente un input a quell’endpoint, si riceverà una previsione generata su quell’input.  \n",
        "- La predizione batch è spesso un’alternativa quando la predizione online è troppo costosa o non abbastanza veloce.  \n",
        "- Con hardware sempre più specializzati e potenti, e con lo sviluppo di tecniche più efficienti per rendere le predizioni online più veloci ed economiche, la predizione online potrebbe diventare lo standard.  \n",
        "\n",
        "**Unificazione delle Pipeline Batch e Streaming**  \n",
        "- La costruzione di infrastrutture che unificano l’elaborazione in streaming e in batch è diventata un tema molto discusso nella comunità del machine learning negli ultimi anni. Aziende come Uber e Weibo hanno effettuato importanti revisioni infrastrutturali per unificare le loro pipeline di elaborazione batch e streaming utilizzando processori di flusso come **Apache Flink**.  \n",
        "- Alcune aziende utilizzano **feature store** per garantire la coerenza tra le caratteristiche batch utilizzate nell'addestramento e le caratteristiche in streaming usate in produzione.\n",
        "\n",
        "**Model Compression**\n",
        "\n",
        "**Low-Rank Factorization**\n",
        "\n",
        "**Knowledge Distillation**\n",
        "\n",
        "**Pruning**\n",
        "\n",
        "**Quantization**\n",
        "\n",
        "\n",
        "**ML nel Cloud e all’Edge**  \n",
        "- Una decisione chiave riguarda dove avverrà l’elaborazione del modello: **nel cloud** o **all’edge**.  \n",
        "  - **Nel cloud** significa che gran parte dell’elaborazione viene eseguita su infrastrutture cloud, sia pubbliche che private.  \n",
        "  - **All’edge** significa che gran parte dell’elaborazione viene eseguita direttamente sui dispositivi dell’utente, come browser, telefoni, laptop, smartwatch, automobili, telecamere di sicurezza, robot, dispositivi embedded, FPGA (field-programmable gate arrays) e ASIC (application-specific integrated circuits), noti anche come **dispositivi edge**.  \n",
        "- Se i modelli sono ospitati su **cloud pubblici**, dipendono da connessioni Internet stabili per inviare e ricevere dati. **L’edge computing**, invece, consente ai modelli di funzionare anche in assenza di connessione o con connessioni instabili, ad esempio in aree rurali o in paesi in via di sviluppo.  \n",
        "- L’elaborazione su cloud implica spesso l’archiviazione di dati di molti utenti nello stesso luogo, aumentando il rischio che una violazione della sicurezza possa colpire più persone.  \n",
        "\n",
        "**Compilazione e Ottimizzazione dei Modelli per Dispositivi Edge**  \n",
        "- **Ottimizzazione dei modelli**  \n",
        "- **Uso del Machine Learning per ottimizzare i modelli ML**  \n",
        "- **ML nei browser**  "
      ],
      "metadata": {
        "id": "5TD94YvnELk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Data Distribution Shifts and Monitoring\n",
        "## 9. Continual Learning and Test in Production\n",
        "## 10. Infrastructure and Tooling for MLOps\n",
        "## 11. The Human Side of Machine Learning"
      ],
      "metadata": {
        "id": "3zsWbSybEMff"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80Ws-vIohu29"
      },
      "outputs": [],
      "source": []
    }
  ]
}