{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN4MOjoN369egnVkViahXDc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PozzOver13/learning/blob/main/book_review/20250220_design_machine_learning_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Design Machine Learning Systems by Chip Huyen"
      ],
      "metadata": {
        "id": "Gwt5T2QWj12c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Chip Huyen\n",
        "\n",
        "Chip Huyen è una **scrittrice e computer scientist specializzata in machine learning e MLOps**. Ha lavorato presso NVIDIA e Snorkel AI e ha co-fondato Claypot AI, una startup focalizzata sull’implementazione di modelli di machine learning in tempo reale. Inoltre, ha **insegnato alla Stanford University** un corso su MLOps e sistemi di ML.\n",
        "\n",
        "Chip Huyen è nota per i suoi post e articoli su blog e social, dove condivide riflessioni su AI, MLOps e ingegneria del software applicata al machine learning."
      ],
      "metadata": {
        "id": "F8iyi8Egh6eP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Overview of Machine Learning Systems\n",
        "\n",
        "![ml_system](https://drive.google.com/uc?id=1duHY57mA7AiXmcK6sWMO2JcJiOZVdyEk)\n",
        "\n",
        "Il libro inizia presentando una definizione di un sistema di Machine Learning (ML) e proponendosi come framework per costruirlo in modo efficace.\n",
        "Ogni data scientist deve sempre ricordare che l'algoritmo e lo sviluppo di un modello predittivo rappresenta solo una piccola parte di un \"sistema di Machine Learning\". Per definirlo è possibile ragionare su 6 macro categorie presentate in questo libro, quali:\n",
        "1. Infrastructure\n",
        "1. Data\n",
        "1. Feature Engineering\n",
        "1. ML algorithms\n",
        "1. Evaluation Metrics\n",
        "1. Deployment, monitoring, update logics\n",
        "\n",
        "Questo sistema interagisce con diverse entità, come gli stakeholders di business, gli utenti e in primis con gli sviluppatori. Ognuno di questi attori porta con se i propri requisiti, che devono essere necessariamente allineati per permettere ad un sistema di ML di prosperare. Questa è sicuramente una sfida ricorrente e che solo le realtà più efficienti riesce a gestire opportunamente.\n",
        "\n",
        "Il rischio per i data scientist è di concentrarsi troppo sulla parte di sviluppo mentre potrebbe essere portato a trascurare gli aspetti di deployment o monitoring. Un esempio ricorrente è rappresentato dalle sfide computazionali e dai requisiti di latenza che potrebbero essere anche altamente vincolanti per alcuni clienti. Altri elementi forse più contigui allo sviluppo come la \"fairness\" e la \"interpretability\" di un modello si stanno affermando come requisiti necessari e vincolanti per la buona riuscita di un sistema ML. Una citazione che mi è sembrata particolarmente interessante è la seguente:\n",
        "\n",
        "> Gli algoritmi di ML non prevedono il futuro, ma codificano il passato, perpetuando così i bias presenti nei dati e oltre. Quando vengono implementati su larga scala, gli algoritmi di ML possono discriminare le persone su larga scala.\n",
        "\n",
        "Questo è un principio che occorre sempre ricordare ma che è diventato molto più evidente con l'introduzione di prodotti costruiti sui Large Language Models (LLM), che per loro costruzione metodologica sintetizzano i pregi e i difetti dei dati sui cui sono stati addestrati. Questa criticità sottolinea ancora in modo più evidente come occorra distringuere i modelli di machine learning rispetto ai software tradizionali. Non basta un codice funzionante ed efficiente ma occorre una perfetta sincronizzazione tra codice, dati e artefatti. Per questo il ruolo del data scientist è particolarmente importante sia per la responsabilità sullo sviluppo software ma soprattutto per la creazione di artefatti performanti e adattabili a delle condizioni potenzialmente in rapido mutamento e, mi permetto di aggiungere, per la diffusione della cultura del dato. Infatti, sono numerosi gli esempi che testimoniano come un aumento della quantità e qualità dei dati permette di ottenere un vantaggio competitivo rilevante.\n",
        "\n"
      ],
      "metadata": {
        "id": "LL7uX1Cb0qhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Introduction to Machine Learning Systems Design\n",
        "\n",
        "![ml_lifecycle](https://drive.google.com/uc?id=1hTIX1oxNwDN8CvWDEw3tUY33-RUhcjk9)\n",
        "\n",
        "Una volta definito dall'alto le componenti di un sistema di machine learning occorre passare alla parte più operativa. Questa parte operativa si suddivide in ciclo di attività connesse tra di loro, la cosiddetta **\"machine learning lifecycle\"**.   \n",
        "**Incrementare fatturato, ridurre i costi e migliorare i margini** sono evidentemente gli unici veri obbiettivi delle aziende. Tuttavia, non è possibile creare un unico sistema che gestisca tutti questi obbiettivi quindi solitamente vengono definiti obbiettivi più specifici e connessi a singoli prodotti o funzioni, che quindi hanno i loro specifici obbiettivi. La parte interessante però si trova nel **ritorno degli investimenti (ROI)**: questa metrica dipende fortemente dalla maturità nell'adozione di questi sistemi perché influenzerà l'efficienza della pipeline, la rapidità del ciclo di sviluppo, del tempo di ingegneria necessario e i costi del cloud, il che porta a ritorni più elevati.\n",
        "\n",
        "$ ROI = Pipeline(rapidità\\_sviluppo, tempo\\_ingegneria, costi\\_cloud) $\n",
        "\n",
        "Per definire un sistema di machine learning soddisfacente occorre che soddisfi 4 principi: **Affidabilità, Scalabilità, Manutenibilità, Adattabilità**. Questo significa che un sistema dovrebbe ridurre al limite i malfunzionamenti dovuti ad hardware o software ed errori umani. Dovrebbe essere flessibile per poter gestire carichi maggiori, maggiori complessità e un maggiore numero di modelli predittivi, spesso però questa parte è collegata alla scelta del cloud provider. Successivamente, scegliere strumenti adatti al monitoraggio, versionamenti di codice, dati e artefatti, oltre ad una documentazione efficiente e dei processi tra diversi team che non creino frizioni e ritardi. Infine, la particolarità di questi sistemi risiede nella possibilità di evolvere al cambiamento del contesto in cui operano soprattutto quelli particolarmente dinamici.\n",
        "\n",
        "Per un data scientist la parte spesso sottovalutata la parte di **problem framing** in quanto le richieste di business possono essere ampie e imprecise a piacere ma è responsabilità degli sviluppatori trasformare la richiesta in un problema di data science o in alternativa bloccare gli sviluppi per mancanza di elementi o dati. Un altro punto rilevante in questo passaggio è la necessità di considerare i problemi di machine learning come problemi articolati che spesso conviene scomporre in sotto problemi (**Decoupling degli Obiettivi**) in modo da poterli affrontare più agilmente, scalarli a persone diverse del team e successivamente gestirli più comodamente.\n",
        "\n",
        "Quello che invece va ricordato costantemente è che la **qualità e quantità dei dati** sono spesso la chiave dei sistemi migliori. Sicuramente il valore aggiunto di un data scientist è alto ma i dati sono il carburante per alimentare anche i motori migliori. Il tema della quantità dei dati e della sua rilevanza è particolarmente evidente ora con i recenti sviluppi degli LLM. Tuttavia, è un **dibattito** che durerà per sempre e che occorre continuare a monitorare ed essere capaci a declinare in base alla realtà in cui si opera.\n",
        "\n",
        "  > \"I dati sono profondamente stupidi.\"  \n",
        "  *Dr. Judea Pearl, vincitore del Turing Award*\n",
        "\n",
        "  > \"Non abbiamo algoritmi migliori. Abbiamo solo più dati.\"  \n",
        "  *Peter Norvig, direttore della ricerca di Google*\n",
        "  \n",
        ">  **\"Senza dati, non esiste data science.\"**"
      ],
      "metadata": {
        "id": "AiunTjJe0xb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Engineering Fundamentals\n",
        "\n",
        "![etl_process](https://drive.google.com/uc?id=1fUfU5Ap5qmNZl5t2JZYabnGVmgswAwfw)\n",
        "\n",
        "\n",
        "- Se vuoi approfondire l'ingegneria dei dati da una prospettiva sistemistica, ti consiglio l'eccellente libro di Martin Kleppmann *Designing Data-Intensive Applications* (O’Reilly, 2017).  \n",
        "\n",
        "**Fonti di Dati**  \n",
        "- I dati di **prima parte** sono quelli che la tua azienda raccoglie direttamente sugli utenti o clienti.  \n",
        "- I dati di **seconda parte** sono raccolti da un'altra azienda sui propri clienti e messi a disposizione (spesso a pagamento).  \n",
        "- I dati di **terza parte** sono raccolti da aziende che monitorano il pubblico senza avere una relazione diretta con esso.  \n",
        "\n",
        "**Formati di Dati**  \n",
        "- Il processo di conversione di una struttura dati o dello stato di un oggetto in un formato archiviabile o trasmissibile, ricostruibile in seguito, è chiamato **serializzazione dei dati**. Esistono moltissimi formati di serializzazione, e nella scelta bisogna considerare leggibilità umana, pattern di accesso, e se il formato è testuale o binario (impattando sulle dimensioni dei file).  \n",
        "- Wikipedia ha un'ottima pagina intitolata *Comparison of Data-Serialization Formats*.  \n",
        "\n",
        "**JSON**  \n",
        "- JSON (*JavaScript Object Notation*) è onnipresente. Pur derivando da JavaScript, è indipendente dal linguaggio: la maggior parte dei linguaggi moderni può generarlo e analizzarlo. È leggibile dall’uomo e utilizza un paradigma chiave-valore potente e adatto a dati con diversi livelli di struttura.  \n",
        "\n",
        "**Formato Row-Major vs Column-Major**  \n",
        "- Due formati comuni, che rappresentano due paradigmi distinti, sono **CSV** e **Parquet**.  \n",
        "  - **CSV** è **row-major**, ossia gli elementi consecutivi di una riga sono memorizzati insieme in memoria.  \n",
        "  - **Parquet** è **column-major**, ossia gli elementi consecutivi di una colonna sono memorizzati insieme.  \n",
        "- I formati **row-major** sono migliori per scritture frequenti, mentre quelli **column-major** sono più efficienti nelle letture colonnari.  \n",
        "- Accedere a un DataFrame per riga è molto più lento che accedervi per colonna. Tuttavia, convertendo lo stesso DataFrame in un `ndarray` di NumPy, l’accesso per riga diventa molto più veloce.  \n",
        "\n",
        "**Formato Testuale vs Binario**  \n",
        "- **CSV e JSON** sono file di **testo**, mentre **Parquet** è un file **binario**.  \n",
        "- I file testuali sono leggibili dall’uomo, mentre i file binari sono destinati a essere interpretati da programmi che leggono byte grezzi.  \n",
        "- **AWS consiglia il formato Parquet** perché permette uno scaricamento fino a **2 volte più veloce** e consuma fino a **6 volte meno spazio** rispetto ai formati testuali su Amazon S3.  \n",
        "\n",
        "**Modelli di Dati**  \n",
        "**Modello Relazionale**  \n",
        "- I dati sono organizzati in **relazioni** (insiemi di tuple). Una tabella è la rappresentazione visiva di una relazione, e ogni riga è una tupla.  \n",
        "- La **normalizzazione** sparge i dati tra più relazioni: questo riduce la ridondanza ma rende costose le operazioni di join su tabelle grandi.  \n",
        "- **SQL è un linguaggio dichiarativo**, a differenza di Python che è imperativo.  \n",
        "  - In un linguaggio **imperativo**, si specificano i passaggi per ottenere un risultato.  \n",
        "  - In un linguaggio **dichiarativo**, si specifica solo il risultato desiderato e il sistema trova i passaggi necessari.  \n",
        "- Esempi di framework ML dichiarativi: **Ludwig** (Uber) e **H2O AutoML**.  \n",
        "\n",
        "**NoSQL**  \n",
        "- Inizialmente il termine NoSQL era un hashtag per incontri sui database non relazionali, poi è stato reinterpretato come *Not Only SQL*.  \n",
        "- I due principali modelli NoSQL sono:  \n",
        "  - **Modello a documenti**, utile quando i dati sono contenuti in documenti indipendenti con relazioni rare.  \n",
        "  - **Modello a grafo**, utile quando le relazioni tra elementi sono comuni e importanti.  \n",
        "\n",
        "**Modello a Documenti**  \n",
        "- Nei database a documenti, la struttura viene determinata dal sistema che legge i dati anziché da quello che li scrive.  \n",
        "- Molti database, come **PostgreSQL e MySQL**, supportano sia il modello relazionale che quello a documenti.  \n",
        "\n",
        "**Modello a Grafo**  \n",
        "- Usato per dati con molte relazioni complesse e connesse.  \n",
        "\n",
        "**Dati Strutturati vs Non Strutturati**  \n",
        "- Un **data warehouse** memorizza dati strutturati.  \n",
        "- Un **data lake** conserva dati non strutturati (spesso grezzi, da processare successivamente).  \n",
        "\n",
        "**Motori di Archiviazione e Elaborazione**  \n",
        "**Elaborazione Transazionale vs Analitica**  \n",
        "- **Database transazionali** sono progettati per gestire transazioni con **bassa latenza e alta disponibilità**.  \n",
        "- Quando si parla di database transazionali, si pensa subito alle proprietà **ACID** (Atomicità, Consistenza, Isolamento, Durabilità).  \n",
        "- **OLTP vs OLAP**  \n",
        "  - **OLTP** (Online Transaction Processing): ottimizzato per operazioni rapide e frequenti su pochi record.  \n",
        "  - **OLAP** (Online Analytical Processing): ottimizzato per analisi su grandi quantità di dati.  \n",
        "\n",
        "**ETL: Extract, Transform, Load**  \n",
        "- ETL è il processo generale di estrazione, trasformazione e caricamento dei dati nel formato desiderato.  \n",
        "\n",
        "**Modalità di Flusso dei Dati**  \n",
        "**Dati che Passano Attraverso Database**  \n",
        "- Entrambi i processi accedono ai dati via database, ma letture/scritture possono essere lente e inadatte per applicazioni con requisiti di latenza stretti (come le app consumer).  \n",
        "\n",
        "**Dati che Passano Attraverso Servizi**  \n",
        "- Due principali stili di richiesta:  \n",
        "  - **REST** (*Representational State Transfer*): progettato per richieste su rete.  \n",
        "  - **RPC** (*Remote Procedure Call*): cerca di rendere una richiesta di rete simile a una chiamata a funzione locale.  \n",
        "- **REST è dominante nelle API pubbliche**, mentre **RPC** è più usato per richieste interne tra servizi della stessa organizzazione.  \n",
        "\n",
        "**Dati che Passano Attraverso Trasporto in Tempo Reale**  \n",
        "- Invece di usare database come intermediari, si usa la **memoria**.  \n",
        "- Il **trasporto in tempo reale** può essere visto come una memoria per lo scambio di dati tra servizi.  \n",
        "- Un dato trasmesso nel sistema si chiama **evento**, per questo l’architettura è anche detta **event-driven**.  \n",
        "- Un sistema di trasporto in tempo reale è chiamato **event bus**.  \n",
        "\n",
        "**Elaborazione in Batch vs Stream Processing**  \n",
        "- **Batch processing**: l’elaborazione avviene su dati accumulati in blocchi.  \n",
        "  - Sistemi come **MapReduce e Spark** sono progettati per elaborare dati batch in modo efficiente.  \n",
        "- **Stream processing**: i dati vengono elaborati in tempo reale mentre fluiscono attraverso sistemi come **Apache Kafka e Amazon Kinesis**.  \n",
        "  - Può essere eseguito periodicamente, ma con intervalli più brevi rispetto al batch processing.  "
      ],
      "metadata": {
        "id": "ENyP8MsEiaI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training Data"
      ],
      "metadata": {
        "id": "WL1DAMu1KuYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Feature Engineering\n",
        "\n",
        "![feat_eng](https://drive.google.com/uc?id=1AwpVue_BJZRefmLDu2-ZuykNXcz_S3gR)\n",
        "\n",
        "\n",
        "La parte dedicata alla feature engineering è una parte interessante perché come spesso accade una delle difficoltà è cercare di mappare dall'altro questa fase di sviluppo che è parte integrante dello sviluppo di un servizio di machine learning. Spesso si fatica a decidere dove posizione questa fase. In questo libro viene posizionato tra la parte di dati (data engineering e training data) e quella di model development. Su questa suddivisione sono particolarmente d'accordo anche se bisogna sempre ricordare che si tratta di un grafo di passaggi che può comportare delle reiterazioni del ciclo al fine di ottenere i risultati migliori.\n",
        "\n",
        "La mappatura prevede:\n",
        "1. **Gestione dei Valori Mancanti**: anche all'interno di questa categoria si affrontano differenze di valori mancanti che vengono mappati in relazione alla natura (non casualità MNAR, casualità condizionata MAR, completa casualità MCAR). Successivamente vengono affrontate le tecniche di eliminazione e di imputazione\n",
        "1. **Scaling**: qui la classica suddivisione tra normalizzazione e standardizzazione\n",
        "1. **Discretizzazione**: categorizzazione delle variabili sempre ricordando il trade-off che porta potenzialmente ai limiti nei punti di discontinuità delle classi e la perdita di granularità del dato.\n",
        "1. **Encoding variabili categoriche**: trasformazione numerica delle variabili categoriche, in questa sezione devo approfondire il \"hashing trick\", reso popolare dal pacchetto Vowpal Wabbit sviluppato da Microsoft.  \n",
        "1. **Feature Crossing**: tecnica per combinare due o più feature e generare nuove feature. Questa tecnica è utile per modellare le relazioni non lineari tra le variabili.\n",
        "1. **Embedding posizionali**: le trasformazioni tramite embedding, sia esso posizionale discreto o continuo tramite le \"fourier features\"-\n",
        "\n",
        "Tuttavia, la parte più interessante perché non ci ho mai riflettuto approfonditamente è il fenomeno definito come **\"Data Leakage\"**\n",
        "\n",
        "> Il data leakage si riferisce al fenomeno in cui una forma dell’etichetta “trapela” nel set di feature utilizzate per fare previsioni, mentre questa stessa informazione non è disponibile durante l’inferenza\n",
        "\n",
        "Con questo tema quindi ci si riferisce a tutte quelle volte in cui il target viene incorporato senza controllo nelle features e quindi nei modelli, portando di conseguenze pericolose in applicazione quando questo effetto era stato sottovalutato. Esempi negativi sono i modelli utilizzati durante il covid per prevedere le future evoluzioni, oppure nella competizione di Kaggle \"Ion Switching\" indetta dalla Università di Liverpool in cui i partecipanti erano riusciti a fare reverse engineering e ottenere le label del test. Sempre nell'amnbito di Kaggle come dimenticare Pavel Pleskov, che era stato sospeso proprio per lo stesso punto.\n",
        "\n",
        "Non esiste un metodo infallibile per evitare questo tipo di fuga di dati, ma si può mitigare il rischio tenendo traccia delle fonti dei dati e comprendendo come vengono raccolti e processati. Questo mi ricorda ancora quanto sia importante dedicare tempo all'analisi e all'approfondimento, meglio se congiuntamente con **esperti di dominio (SME)** per capire il dato che si sta analizzando. A questo fine tornano utili anche le attività di **feature importance e feature generalization**.\n",
        "\n",
        "> L’algoritmo esatto per misurare l’importanza delle feature è complesso, ma intuitivamente, l’importanza di una feature in un modello viene misurata in base a quanto peggiora la performance del modello se quella feature o un set di feature contenente quella feature viene rimosso"
      ],
      "metadata": {
        "id": "s4QzO3WEKu8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Model Development and Offline Evaluation\n",
        "\n",
        "**Sviluppo e Addestramento del Modello**\n",
        "\n",
        "> \"tutti i modelli sono sbagliati, ma alcuni sono utili\" George Box (1976)\n",
        "\n",
        "Siamo finalmente arrivati alla parte divertente per qualsiasi data scientist. Infatti, è difficile trovare spunti mai sentiti però è utile per ragionare sempre in ottica di framework. Infatti, partiamo dal punto che la valutazione dei modelli deve partire dalle metriche di performance dipendenti dal target deciso (classificazione o regressione) tuttavia non bisogna tralasciare altre proprietà: tra queste potrebbero esserci la quantità di dati, risorse computazionali, tempo per addestramento, latenza in ottica di inferenza e interpretabilità. Questo punto è coerente con l'approccio di tutto il libro che volontariamente tralascia gli aspetti più metodologici per mappare i sistemi di ML in modo omnicomprensivo. Interessante però che vengano sottolineati le conferenze più interessanti (NeurIPS, ICLR e ICML) o tecnici attivi sui social per rimanere aggiornati sulle metodologie stato dell'arte.\n",
        "Vengono proposti dall'autrice dei consigli (5) per migliorare la selezione di un modello. Viene consigliato di partire sempre da modelli più semplici per avere dei benchmark di valutazione senza complicare la parte di deployment in modo potenzialmente non necessario, ma anche valutare accuratamente i potenziali trade off come falsi negativi o positivi, e i limiti dovuti alle assunzioni dei modelli. Un aspetto che mi ha colpito e che faccio poco è provare a fare dei sample di dati e misurare la performance sia sulla totalità del campione sia su campioni distorti; questa parte viene riprodotta in parte dal classico split train-validation-test però occorre ricordarlo e potenzialmente estremizzare questo aspetto al fine di averne una misura effettiva della capacità predittiva del modello a condizioni variabili (campioni out of sample and out of time). Un aspetto umano che invece si consiglia di non trascurare è la possibilità di preferenze personali aprioristiche che potrebbero influenzare il numero di esperimenti con un dato modello (\"xgboost funziona sempre bene e quindi faccio tutti i test possibili di questo mentre tralascio metodologie che ritengo meno efficaci\").\n",
        "\n",
        "Empiricamente all'interno delle organizzazioni e soprattutto delle competizioni Kaggle e similari le metodologie di \"ensemble\" garantiscono performance migliori rispetto a modelli semplici, quindi è bene conoscerli in modo approfondito per applicarli in modo ottimale.\n",
        "Tecniche comuni:\n",
        "- **Bagging**: vengono creati dei sotto campioni con rimpiazzo (boostraps) e su questi vengono addestrati modelli separati che poi vengono aggregati (voto per maggioranza, media e altre metodologie)\n",
        "- **Boosting**: algoritmi iterativi che utilizzano sempre gli stessi dati ma i base learner vengono addestrati per ridurre gli errori dei precedenti e vengono pesati in modo decrescente\n",
        "- **Stacking**: quando è possibile risulta un'ottima soluzione stimare modelli diversi e se sufficientemente non correlati unirli in un passaggio successivo attraverso euristiche o un meta learner che potrebbe essere un modello aggiuntivo.\n",
        "\n",
        "La parte più interessante del capitolo riguarda la parte di experiment tracking e versioning. Il processo di monitoraggio dei progressi e dei risultati di un esperimento si chiama **tracciamento degli esperimenti**. Il processo di registrare tutti i dettagli di un esperimento, per poterlo eventualmente ricreare o confrontare con altri esperimenti, si chiama **versionamento**. Questa parte è interessante perché non esiste uniformità su come si possa strutturare questa parte. Ci sono degli strumenti che semplificano una delle due parti e per estensione poi fanno anche una parte della seconda ma non esiste univocità su come farlo e su cosa considerare come differenza tra due dati: basta una riga? o solo se il dato viene cambiato nella struttura o viene eliminato?\n",
        "\n",
        "In ogni caso occorre sempre ricordare che la complessità dei sistemi di ML si base sempre sulla necessità di tenere allineati tutti gli elementi coinvolti, che spesso afferiscono anche a team diversi.\n",
        "\n",
        "Il capitolo poi tratta dei temi che mi stanno poco a cuore per la distanza rispetto alle mie attività classica, ovvero il data and model parallelism. Queste sono metodologie, o forse le definirei più architetture, per risolvere problemi di dimensionalità dei dati nei casi in cui i dati e i modelli non possano essere gestiti nei limiti della memoria computazionale disponibile. Allo stesso modo anche la parte di autoML è un argomento interessante ma che andrebbe approfondito maggiormente per capire quali possano essere le applicazioni utili, ad esempio viene citata l'ottimizzazione degli iper parametri che è oggettivamente un'attività lunga e priva di valore aggiunto umano.\n",
        "\n",
        "Infine, vengono ricordati i rischi connessi alla valutazione di un modello offline rispetto alla produzione. Prima di eseguire il deploy di un modello è sempre bene testare dei modelli di riferimento (baseline) in modo da poter rispondere facilmente a domande che a volte si tralasciano per la loro potenziale semplicità. Quindi cosa dovremmo testare? In primis, modelli casuali e predizione generate da semplici euristiche come ad esempio predire sempre la classe più frequente; oppure, in caso sia disponibile è sempre interessante la performance di un modello di machine learning rispetto ad una valutazione umana, mentre se è disponibile una versione precedente del modello è obbligatorio confrontare le performance per capire se sono stati fatti effettivamente dei miglioramenti.\n",
        "Oltre ad ampliare il set di possibili predizioni contro cui testare i modelli è bene anche stressare questi modelli in modo da essere sicuri che i modelli siano robusti, fair e calibrati correttamente. In questo caso sono titoli interesanti per avere un framework di attività che spesso di fanno ma non vengono codificate in modo univoco:\n",
        "1. **perturbation test**: aggiungere rumore rispetto ai dati di training\n",
        "1. **inviariance test**: controllare che singole variabili non siano discriminatorie (es: razza)\n",
        "1. **directional expectation tests**: al crescere della dimensione degli immobili il valore complessivo non può mai diminuire\n",
        "1. **model calibration**: controllare l'effettiva calibrazione (questo è un passaggio da approfondire in quanto disponibile anche in scikit-learn)\n",
        "1. **confidence measurement**: cercare di dare delle misure di confidenze per potenzialmente filtrare predizioni su cui non siamo confidenti\n",
        "1. **slice based evaluation**: occorre ricordarsi sempre del Simpson's Paradox (fenomeno per cui i trend e le performance sul campione complessivo possano ribaltarsi completamente nei sottogruppi) e quindi testare sempre le performance in modo complessivo ma anche sui sottogruppi. I sottogruppi possono essere definiti per conoscenza di business, tramite analisi dell'errore e attraverso metodologie di scoperta automatica di sottogruppi sviluppate negli ultimi anni."
      ],
      "metadata": {
        "id": "yKlnfR7QWkIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Model Deployment and Prediction Service\n",
        "\n",
        "**Introduzione**  \n",
        "- In molte aziende, la responsabilità di distribuire i modelli ricade sugli stessi sviluppatori che li hanno creati. In molte altre, una volta che un modello è pronto per il deployment, viene esportato e consegnato a un altro team per l’implementazione. Tuttavia, questa separazione di responsabilità può causare un’elevata comunicazione tra i team e rallentare l’aggiornamento del modello.  \n",
        "- Il processo di generazione delle previsioni è chiamato **inferenza**.  \n",
        "\n",
        "**Miti sul Deployment dei Modelli di Machine Learning**  \n",
        "**Mito 1: Si Distribuiscono Solo Uno o Due Modelli ML alla Volta**  \n",
        "- In realtà, Uber ha migliaia di modelli in produzione. In qualsiasi momento, Google ha migliaia di modelli in fase di addestramento, alcuni con centinaia di miliardi di parametri. Booking.com utilizza oltre 150 modelli. Uno studio del 2021 di Algorithmia mostra che tra le organizzazioni con più di 25.000 dipendenti, il 41% ha più di 100 modelli in produzione.  \n",
        "\n",
        "**Mito 2: Se Non Facciamo Nulla, le Prestazioni del Modello Rimangono Invariate**  \n",
        "- I sistemi di machine learning soffrono di quelli che vengono chiamati **cambiamenti nella distribuzione dei dati**, ovvero situazioni in cui la distribuzione dei dati incontrata in produzione è diversa da quella su cui il modello è stato addestrato.  \n",
        "\n",
        "**Mito 3: Non È Necessario Aggiornare Spesso i Modelli**  \n",
        "- Poiché le prestazioni di un modello decadono nel tempo, è importante aggiornarlo il più rapidamente possibile. Questo è un aspetto del ML in cui possiamo imparare dalle migliori pratiche del DevOps. Già nel 2015, aziende come Etsy rilasciavano aggiornamenti 50 volte al giorno, Netflix migliaia di volte al giorno e AWS ogni 11,7 secondi.  \n",
        "\n",
        "**Mito 4: La Maggior Parte degli Ingegneri ML Non Deve Preoccuparsi della Scalabilità**  \n",
        "\n",
        "**Predizione in Batch vs. Predizione Online**  \n",
        "- Una decisione fondamentale che influenzerà sia gli utenti finali che gli sviluppatori è il modo in cui il sistema genera e serve le previsioni: **online** o **batch**.  \n",
        "- Tradizionalmente, nella predizione online, le richieste vengono inviate al servizio di previsione tramite API RESTful (es. richieste HTTP). Quando le richieste di previsione vengono inviate via HTTP, si parla anche di **predizione sincrona**, poiché le previsioni vengono generate in tempo reale rispetto alle richieste.  \n",
        "- La **predizione batch** è anche nota come **predizione asincrona**, poiché le previsioni vengono generate in modo asincrono rispetto alle richieste.  \n",
        "- I termini \"predizione online\" e \"predizione batch\" possono creare confusione, perché entrambi possono elaborare più campioni alla volta (batch) o un campione per volta. Per evitare questa ambiguità, a volte si preferiscono i termini **\"predizione sincrona\"** e **\"predizione asincrona\"**.  \n",
        "\n",
        "**Dalla Predizione Batch alla Predizione Online**  \n",
        "- Si esporta il modello, lo si carica su Amazon SageMaker o Google App Engine e si ottiene un endpoint esposto. Ora, se si invia una richiesta contenente un input a quell’endpoint, si riceverà una previsione generata su quell’input.  \n",
        "- La predizione batch è spesso un’alternativa quando la predizione online è troppo costosa o non abbastanza veloce.  \n",
        "- Con hardware sempre più specializzati e potenti, e con lo sviluppo di tecniche più efficienti per rendere le predizioni online più veloci ed economiche, la predizione online potrebbe diventare lo standard.  \n",
        "\n",
        "**Unificazione delle Pipeline Batch e Streaming**  \n",
        "- La costruzione di infrastrutture che unificano l’elaborazione in streaming e in batch è diventata un tema molto discusso nella comunità del machine learning negli ultimi anni. Aziende come Uber e Weibo hanno effettuato importanti revisioni infrastrutturali per unificare le loro pipeline di elaborazione batch e streaming utilizzando processori di flusso come **Apache Flink**.  \n",
        "- Alcune aziende utilizzano **feature store** per garantire la coerenza tra le caratteristiche batch utilizzate nell'addestramento e le caratteristiche in streaming usate in produzione.\n",
        "\n",
        "![ml_workflow](https://drive.google.com/uc?id=1tfjNSt2OtvHvEuX4wiklGW96KdgLhxiv)\n",
        "\n",
        "**Model Compression**\n",
        "\n",
        "**Low-Rank Factorization**\n",
        "\n",
        "**Knowledge Distillation**\n",
        "\n",
        "**Pruning**\n",
        "\n",
        "**Quantization**\n",
        "\n",
        "\n",
        "**ML nel Cloud e all’Edge**  \n",
        "- Una decisione chiave riguarda dove avverrà l’elaborazione del modello: **nel cloud** o **all’edge**.  \n",
        "  - **Nel cloud** significa che gran parte dell’elaborazione viene eseguita su infrastrutture cloud, sia pubbliche che private.  \n",
        "  - **All’edge** significa che gran parte dell’elaborazione viene eseguita direttamente sui dispositivi dell’utente, come browser, telefoni, laptop, smartwatch, automobili, telecamere di sicurezza, robot, dispositivi embedded, FPGA (field-programmable gate arrays) e ASIC (application-specific integrated circuits), noti anche come **dispositivi edge**.  \n",
        "- Se i modelli sono ospitati su **cloud pubblici**, dipendono da connessioni Internet stabili per inviare e ricevere dati. **L’edge computing**, invece, consente ai modelli di funzionare anche in assenza di connessione o con connessioni instabili, ad esempio in aree rurali o in paesi in via di sviluppo.  \n",
        "- L’elaborazione su cloud implica spesso l’archiviazione di dati di molti utenti nello stesso luogo, aumentando il rischio che una violazione della sicurezza possa colpire più persone.  \n",
        "\n",
        "**Compilazione e Ottimizzazione dei Modelli per Dispositivi Edge**  \n",
        "- **Ottimizzazione dei modelli**  \n",
        "- **Uso del Machine Learning per ottimizzare i modelli ML**  \n",
        "- **ML nei browser**  "
      ],
      "metadata": {
        "id": "5TD94YvnELk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Data Distribution Shifts and Monitoring\n",
        "## 9. Continual Learning and Test in Production"
      ],
      "metadata": {
        "id": "3zsWbSybEMff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 10. Infrastructure and Tooling for MLOps\n",
        "\n",
        "![infrastructure](https://drive.google.com/uc?id=147yWgCiShtZoEv0eKABX9aZNrfMu1hMC)\n",
        "\n",
        "\n",
        "Nel mondo del ML, l'infrastruttura è l'insieme delle strutture fondamentali che supportano lo sviluppo e la manutenzione dei sistemi di ML.**  \n",
        "\n",
        "**Storage e Compute**  \n",
        "- Nella sua forma più semplice, lo strato di storage può essere un hard disk (HDD) o un disco a stato solido (SSD). Può essere centralizzato, ad esempio con tutti i dati in Amazon S3 o Snowflake, oppure distribuito su più posizioni. Lo storage può essere on-premise in un data center privato o nel cloud.  \n",
        "- Lo strato di calcolo (compute) si riferisce a tutte le risorse computazionali a cui un'azienda ha accesso e al meccanismo per determinarne l'utilizzo.  \n",
        "- Nella sua forma più semplice, lo strato di compute può essere un singolo core CPU o GPU. La forma più comune è il calcolo gestito da un provider cloud come AWS EC2 o GCP.  \n",
        "- Alcuni motori di calcolo, come Spark e Ray, utilizzano \"job\" come unità di computazione, mentre Kubernetes utilizza i \"pod\", che sono contenitori eseguibili.  \n",
        "- Un'unità di calcolo si caratterizza principalmente per due metriche: la quantità di memoria disponibile e la velocità di esecuzione delle operazioni.  \n",
        "\n",
        "**Cloud pubblico vs Data Center privato**  \n",
        "- Il cloud permette alle aziende di iniziare rapidamente senza preoccuparsi dell'infrastruttura. È particolarmente vantaggioso per carichi di lavoro variabili.  \n",
        "- Il cloud semplifica la scalabilità delle risorse computazionali, riducendo il carico operativo degli ingegneri. Questo è utile nel ML, dove le esigenze di calcolo variano: durante lo sviluppo gli esperimenti richiedono molta potenza, mentre in produzione il carico è più stabile.  \n",
        "- Secondo un'analisi di a16z, il costo dell'infrastruttura cloud rappresenta circa il 50% dei costi di revenue per molte aziende software.  \n",
        "- a16z ha stimato che 50 tra le principali aziende software pubbliche stanno perdendo circa 100 miliardi di dollari di valore di mercato a causa dell'impatto del cloud sui margini, rispetto a un'infrastruttura gestita internamente.  \n",
        "\n",
        "**Ambiente di sviluppo (Dev Environment)**  \n",
        "L’ambiente di sviluppo include i seguenti componenti: IDE (Integrated Development Environment), versionamento e CI/CD.  \n",
        "\n",
        "**Setup dell’ambiente di sviluppo**  \n",
        "- Le aziende utilizzano strumenti come Git per il versionamento del codice, DVC per il versionamento dei dati, Weights & Biases o Comet.ml per tracciare esperimenti e MLflow per gestire gli artefatti dei modelli in fase di deployment.  \n",
        "- Strumenti per orchestrare la CI/CD includono GitHub Actions e CircleCI.  \n",
        "\n",
        "**IDE**  \n",
        "- I notebook non servono solo per scrivere codice, ma anche per integrare immagini, grafici e tabelle, rendendoli molto utili per l'analisi esplorativa e la valutazione dei risultati di addestramento.  \n",
        "- Netflix, nel suo articolo \"Beyond Interactive: Notebook Innovation at Netflix\", ha mostrato strumenti infrastrutturali per potenziare i notebook.  \n",
        "\n",
        "**Standardizzazione degli ambienti di sviluppo**  \n",
        "- VS Code è una buona scelta perché permette un'integrazione facile con le istanze cloud.  \n",
        "- Alcune aziende utilizzano GitHub Codespaces come ambiente di sviluppo cloud, mentre AWS EC2 o GCP con SSH sono alternative valide.  \n",
        "- Avere l’ambiente di sviluppo nel cloud riduce il divario tra sviluppo e produzione.  \n",
        "\n",
        "**Dallo sviluppo alla produzione: i container**  \n",
        "- Per replicare un ambiente su una nuova istanza, si utilizzano i container. Docker è la tecnologia più diffusa per farlo, permettendo di creare un **Dockerfile** con istruzioni dettagliate per l’ambiente (installazione di pacchetti, modelli pre-addestrati, variabili di ambiente, ecc.).  \n",
        "- Due concetti chiave di Docker:  \n",
        "  - **Image**: l'insieme delle istruzioni del Dockerfile eseguite.  \n",
        "  - **Container**: un’istanza in esecuzione di un'immagine Docker.  \n",
        "- **Orchestrazione dei container**:  \n",
        "  - Docker Compose è un orchestratore leggero per la gestione di più container su un singolo host.  \n",
        "  - Kubernetes (K8s) è uno strumento per orchestrare container su più istanze, permettendo scalabilità e alta disponibilità.  \n",
        "\n",
        "**Gestione delle risorse**  \n",
        "\n",
        "**Cron, Scheduler e Orchestrator**  \n",
        "- Gli scheduler determinano **quando** eseguire un job e quali risorse sono necessarie.  \n",
        "- Gli orchestratori decidono **dove** ottenere tali risorse.  \n",
        "- Kubernetes è l'orchestratore più noto per la gestione dei container.  \n",
        "\n",
        "**Gestione del workflow in Data Science**  \n",
        "- Strumenti come **Airflow, Argo, Prefect, Kubeflow e Metaflow** permettono la gestione dei flussi di lavoro.  \n",
        "- I workflow possono essere definiti come **DAG** (grafi aciclici diretti) e comprendono tipicamente fasi di **featurization, training e valutazione del modello**.  \n",
        "- I workflow possono essere scritti in Python o YAML.  \n",
        "- Ogni fase del workflow è un **task**.  \n",
        "\n",
        "\n",
        "**ML Platform**  \n",
        "\n",
        "**Model Deployment**  \n",
        "- Se il modello esegue **predizioni online**, viene esposto tramite un endpoint che genera una predizione su richiesta.  \n",
        "- Se il modello esegue **predizioni batch**, l’endpoint recupera predizioni precomputate.  \n",
        "- **Soluzioni di deployment più diffuse**:  \n",
        "  - **Cloud provider**: AWS SageMaker, GCP Vertex AI, Azure ML, Alibaba Machine Learning Studio.  \n",
        "  - **Startup e strumenti indipendenti**: MLflow Models, Seldon, Cortex, Ray Serve.  \n",
        "- È importante valutare la facilità di gestione sia per predizioni online che batch.  \n",
        "\n",
        "**Model Store**  \n",
        "- **MLflow è attualmente il model store più popolare** tra quelli non legati a provider cloud.  \n",
        "- Molte aziende hanno capito che archiviare solo il modello non è sufficiente: è fondamentale tracciare anche altre informazioni per il debugging e la manutenzione, tra cui:  \n",
        "  - Definizione del modello  \n",
        "  - Parametri  \n",
        "  - Funzioni di featurization e predizione  \n",
        "  - Dipendenze  \n",
        "  - Dati  \n",
        "  - Codice di generazione del modello  \n",
        "  - Artefatti degli esperimenti  \n",
        "  - Tag  \n",
        "- Il problema del model store è ancora aperto e potrebbe emergere una startup per risolverlo.  \n",
        "\n",
        "**Feature Store**  \n",
        "- Il concetto di \"Feature Store\" varia a seconda di chi lo utilizza. In generale, un Feature Store aiuta a gestire:  \n",
        "  - **Feature Management** (gestione delle feature)  \n",
        "  - **Feature Transformation** (trasformazione delle feature)  \n",
        "  - **Feature Consistency** (coerenza tra training e inferenza)  \n",
        "- Alcuni esempi di strumenti di Feature Store:  \n",
        "  - **Amundsen e Datahub**  \n",
        "  - **Feast, Tecton, SageMaker Feature Store, Databricks Feature Store**  \n",
        "- Un feature store funziona come un **data warehouse** per le feature di ML.  \n",
        "\n",
        "**Build vs Buy**  \n",
        "- **Costruire un ML platform internamente** offre maggiore flessibilità e controllo, ma richiede notevoli investimenti in tempo e risorse.  \n",
        "- **Acquistare soluzioni preconfezionate** può accelerare il time-to-market e ridurre i costi operativi, ma può limitare la personalizzazione.  \n",
        "\n"
      ],
      "metadata": {
        "id": "cwGbtTVMNBil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. The Human Side of Machine Learning\n",
        "\n",
        "**Esperienza Utente**\n",
        "- **Garantire Coerenza nell’Esperienza Utente**\n",
        "    - Per i compiti che vogliono sfruttare il Machine Learning per migliorare l’esperienza degli utenti, l’incoerenza nelle predizioni del modello può rappresentare un ostacolo.\n",
        "  \n",
        "- **Contrastare le Predizioni “Per lo più Corrette”**\n",
        "    - Questo approccio è molto comune e viene talvolta chiamato “AI con l’uomo nel loop” (“human-in-the-loop”), poiché coinvolge esseri umani per selezionare le migliori predizioni o per migliorare quelle generate dalla macchina.\n",
        "\n",
        "- **Degradazione Graduale delle Prestazioni (Smooth Failing)**\n",
        "    - Questo concetto è legato al compromesso tra velocità e accuratezza: un modello potrebbe avere prestazioni inferiori rispetto a un altro, ma essere in grado di effettuare inferenze molto più rapidamente. Questo modello meno ottimale ma più veloce potrebbe fornire predizioni peggiori agli utenti, ma essere comunque preferibile in situazioni in cui la latenza è cruciale.\n",
        "\n",
        "**Struttura del Team**\n",
        "\n",
        "![e2edatascientist](https://drive.google.com/uc?id=1HDN6KrldIuDixWen4_de8yQ9iGW9Zq4J)\n",
        "\n",
        "- **Collaborazione tra Team Cross-funzionali**\n",
        "    - Gli esperti del dominio (SME – Subject Matter Experts) non sono solo utenti, ma anche sviluppatori di sistemi di Machine Learning.\n",
        "    - Un sistema ML trarrebbe grande beneficio dal coinvolgimento degli SME anche nelle fasi successive del ciclo di vita: formulazione del problema, ingegneria delle caratteristiche, analisi degli errori, valutazione del modello, riordinamento delle predizioni e interfaccia utente – ovvero su come presentare al meglio i risultati agli utenti e/o ad altre parti del sistema.\n",
        "\n",
        "- **Data Scientist End-to-End**\n",
        "    - **Approccio 1: Team separato per la gestione della produzione**\n",
        "        - In questo approccio, il team di data science/ML sviluppa i modelli nell’ambiente di sviluppo. Poi un team separato, solitamente il team Ops/piattaforma/ML engineering, mette in produzione i modelli.\n",
        "        - Sfide e svantaggi:\n",
        "            - Sovraccarico nella comunicazione e nel coordinamento\n",
        "            - Difficoltà nel debugging\n",
        "            - Scarico delle responsabilità (scaricabarile)\n",
        "            - Contesto limitato\n",
        "\n",
        "    - **Approccio 2: I data scientist gestiscono l’intero processo**\n",
        "        - In questo approccio, anche la messa in produzione dei modelli è responsabilità del team di data science. I data scientist diventano “unicorni\", a cui si richiede di conoscere tutto il processo, finendo per scrivere più codice ripetitivo (boilerplate) che codice di data science.\n",
        "        - “Il potere del data scientist generalista full-stack e i pericoli della divisione del lavoro per funzione.”\n",
        "        - \"Sono entrato in un’azienda di ML perché volevo passare più tempo con i dati, non con l’avvio di istanze AWS, la scrittura di Dockerfile, la pianificazione e lo scaling dei cluster, o il debugging di file di configurazione YAML.\"\n",
        "        - Secondo Stitch Fix e Netflix, il successo di un data scientist full-stack dipende dagli strumenti che ha a disposizione."
      ],
      "metadata": {
        "id": "tY8y9tc9NCCL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80Ws-vIohu29"
      },
      "outputs": [],
      "source": []
    }
  ]
}